# post-clean check
print(unique(sqf_data$ask_for_consent_flg))
# get % of NAs, in columns with at least one NA
na_cols <- (colMeans(is.na(sqf_data)) * 100)[colMeans(is.na(sqf_data)) * 100 > 0]
# make df for plot
na_cols_df <- data.frame(Feature = names(na_cols), Percentage = na_cols)
# order for plot
na_cols_df$Feature <- factor(na_cols_df$Feature,
levels = na_cols_df$Feature[order(na_cols_df$Percentage, decreasing = FALSE)])
# plot
ggplot(na_cols_df, aes(x = Feature, y = Percentage)) +
geom_bar(stat = "identity", fill = "#F8566D", color = "black") +
labs(title = "Percentage of NA Values per Column",
x = "Columns",
y = "Percentage of NA Values") +
coord_flip() +  # Flip coordinates
theme_minimal()
sqf_data <- sqf_data %>%
select(-all_of(names(na_cols[na_cols > 25])))
dim(sqf_data)
sqf_data <- sqf_data %>%
filter(!if_any(everything(), is.na))
dim(sqf_data)
# pre check
print(unique(sqf_data$firearm_flag))
print(unique(sqf_data$id_card_identifies_officer_flag))
# clean Ys and Ns and set as numeric
sqf_data <- sqf_data %>%
mutate(across(
where(~ all(. %in% c("Y", "N", "I", "V", "S"))),
~ as.numeric(ifelse(. %in% c("Y", "I", "V", "S"), 1, 0))
))
# post check
print(unique(sqf_data$firearm_flag))
print(unique(sqf_data$id_card_identifies_officer_flag))
sqf_data <- sqf_data %>%
mutate(
time_of_day = case_when(
str_extract(stop_frisk_time, "^\\d{2}") %in% c("00", "01", "02", "03", "04", "05") ~ "Late Night",
str_extract(stop_frisk_time, "^\\d{2}") %in% c("06", "07", "08", "09", "10", "11") ~ "Morning",
str_extract(stop_frisk_time, "^\\d{2}") %in% c("12", "13", "14", "15", "16", "17") ~ "Afternoon",
str_extract(stop_frisk_time, "^\\d{2}") %in% c("18", "19", "20", "21", "22", "23") ~ "Evening",
TRUE ~ NA_character_
),
time_of_day = factor(time_of_day, levels = c("Late Night", "Morning", "Afternoon", "Evening"))
)
# check
print(table(sqf_data$time_of_day))
# now drop stop frisk time as we will just use time_of_day
sqf_data <- sqf_data %>%
select(-"stop_frisk_time")
# convert character columns to factors, except for stop location x and y
sqf_data <- sqf_data %>%
mutate(across(
.cols = where(is.character) & !c("stop_location_x", "stop_location_y"),
.fns = as.factor
))
# define convert factor to numeric, handling non-numeric entries
convert_to_numeric <- function(x) {
# convert to character to avoid factor levels issues
x <- as.character(x)
# replace non-numeric values with NA (e.g., "unknown", "760", "7.6")
x <- gsub("[^0-9.]", "", x)
# convert to numeric
as.numeric(x)
}
# apply the function to the relevant columns
sqf_data <- sqf_data %>%
mutate(
suspect_reported_age = convert_to_numeric(suspect_reported_age),
suspect_height = convert_to_numeric(suspect_height),
suspect_weight = convert_to_numeric(suspect_weight)
)
# check to make sure successful
summary(sqf_data$suspect_reported_age)
summary(sqf_data$suspect_height)
summary(sqf_data$suspect_weight)
# Function to convert feet.inches to feet
convert_to_feet <- function(feet_inches) {
# Extract feet (integer part)
feet <- floor(feet_inches)
# Get the fractional part
fractional_part <- feet_inches - feet
# Interpret inches based on the fractional part
if (fractional_part == 0) {
# No fractional part means no additional inches
inches <- 0
} else {
# Convert fractional part to a string to check its length
fractional_str <- as.character(fractional_part)
if (grepl("\\.\\d$", fractional_str)) {
# Case like `.1`: Single digit after decimal -> 1, 2, ..., 9 inches
inches <- fractional_part * 10
} else if (grepl("\\.\\d0$", fractional_str)) {
# Case like `.10`, `.20`, etc.: Two digits ending in `0` -> 10, 20, etc. inches
inches <- fractional_part * 100
} else {
# Case like `.11`, `.12`, etc.: Two digits not ending in `0` -> Exact inches
inches <- fractional_part * 100
}
}
# Validate inches (should be between 0 and 11)
if (inches < 0 || inches > 11.9) {
warning(paste("Invalid height input:", feet_inches, "- Inches must be between 0 and 11.9. Returning NA."))
return(NA) # We have no NAs, so inches were extracted correctly.
}
# Convert inches to feet
inches_in_feet <- inches / 12
# Return total height in feet
return(feet + inches_in_feet)
}
# Apply the conversion function to the 'suspect_height' column
sqf_data$suspect_height_feet <- sapply(sqf_data$suspect_height, convert_to_feet)
# Check the result
tail(sqf_data[, c("suspect_height", "suspect_height_feet")])
sqf_data$suspect_height <- sqf_data$suspect_height_feet
# compute density for suspect_height
height_density <- density(sqf_data$suspect_height, na.rm = TRUE)
# plot the density
plot(height_density,
main = "Density of Height with Outliers Highlighted",
xlab = "Height",
ylab = "Density",
col = "black",
lwd = 2)
grid()
# identify and highlight outliers (e.g., heights below 4 feet and above 7 feet)
outliers <- sqf_data$suspect_height[sqf_data$suspect_height < 4 | sqf_data$suspect_height > 7]
# add vertical lines to mark the outlier boundaries
abline(v = c(4, 7), col = "darkorchid2", lty = 2, lwd = 2)
# highlight the outlier data points on the plot
points(outliers, rep(0, length(outliers)), col = "darkorchid2", pch = 19)
# drop outlier observations where height is above 7 ft and below 4 ft.
sqf_data <- sqf_data[sqf_data$suspect_height >= 4 & sqf_data$suspect_height <= 7, ]
# compute density for reported_age
reported_age_density <- density(sqf_data$suspect_reported_age, na.rm = TRUE)
# plot the density
plot(reported_age_density,
main = "Density of Reported Age with Outliers Highlighted",
xlab = "Reported Age",
ylab = "Density",
col = "black",
lwd = 2)
grid()
# identify and highlight outliers (ages < 10 and > 85)
outliers <- sqf_data$suspect_reported_age[sqf_data$suspect_reported_age < 10 | sqf_data$suspect_reported_age > 85]
# add vertical lines to mark the outlier boundaries
abline(v = c(10, 85), col = "deeppink", lty = 2, lwd = 2)
# highlight the outlier data points on the plot
points(outliers, rep(0, length(outliers)), col = "deeppink", pch = 19)
# drop outlier observations where age is above 85 and below 10.
sqf_data <- sqf_data[sqf_data$suspect_reported_age >= 10 & sqf_data$suspect_reported_age <= 85, ]
#Since the `suspect_reported_age` data is quiet skewed, we will perform a logarithmic transformation.
sqf_data$suspect_reported_age <- log(sqf_data$suspect_reported_age)
# compute density for suspect_weight
weight_density <- density(sqf_data$suspect_weight, na.rm = TRUE)
# plot the density
plot(weight_density,
main = "Density of Suspect Weight with Outliers Highlighted",
xlab = "Weight",
ylab = "Density",
col = "black",
lwd = 2)
grid()
# identify and highlight outliers (e.g., weights below 90 lbs and above 300 lbs)
outliers <- sqf_data$suspect_weight[sqf_data$suspect_weight < 90 | sqf_data$suspect_weight > 300]
# add vertical lines to mark the outlier boundaries
abline(v = c(90, 300), col = "darkturquoise", lty = 2, lwd = 2)
# highlight the outlier data points on the plot
points(outliers, rep(0, length(outliers)), col = "darkturquoise", pch = 19)
# drop outlier observations where height is above 350 lbs. and below 90 lbs.
sqf_data <- sqf_data[sqf_data$suspect_weight >= 90 & sqf_data$suspect_weight <= 300, ]
# standardize the numeric variables
sqf_data <- sqf_data %>%
mutate(
suspect_reported_age = scale(suspect_reported_age),
suspect_height = scale(suspect_height),
suspect_weight = scale(suspect_weight)
)
# check if the standardization worked by summarizing the variables
summary(sqf_data$suspect_reported_age)
summary(sqf_data$suspect_height)
summary(sqf_data$suspect_weight)
# check dim again
dim(sqf_data)
# tabulation of the dependent variable
sqf_data %>%
group_by(suspect_arrested_flag) %>%
summarise(N = n(),
Pc = N / nrow(sqf_data) * 100) %>%
arrange(desc(N)) %>%
kable(booktabs = TRUE, col.names = c("Suspect Arrested", "N Stops", "% Total Stops"), align = "l")
# looking at the distribution of sex
ggplot(sqf_data, aes(x = suspect_sex, fill = suspect_sex)) +
geom_bar() +
labs(
title = "Distribution of Suspect Sex",
x = "Sex",
y = "N Stops"
) +
theme_minimal() +
scale_fill_manual(
values = c("MALE" = "lightblue", "FEMALE" = "pink")
) +
theme(legend.position = "none")
# sex by arrest status
ggplot(sqf_data, aes(x = suspect_sex, fill = factor(suspect_arrested_flag))) +
geom_bar(position = "fill") +
labs(title = "Distribution of Arrest Status, By Suspect Sex",
x = "Sex",
y = "% Stops") +
theme_minimal() +
scale_y_continuous(labels = scales::percent) +
scale_fill_brewer(type = "qual", palette = "Pastel2", name = "Suspect Arrested")
# empirical cdf of age by sex and arrest status
ggplot(sqf_data, aes(x = suspect_reported_age, color = factor(suspect_arrested_flag))) +
stat_ecdf(geom = "step") +
facet_wrap(~ suspect_sex, ncol = 2) +
scale_color_manual(values = c("0" = "red", "1" = "darkgreen"),
labels = c("Not Arrested", "Arrested"),
name = "Arrest Outcome") +
labs(x = "Suspect Reported Age", y = "ECDF", title = "Empirical CDF of Suspect Reported Age, By Sex and Arrest Status") +
theme_minimal()
# arrests by race, unstacked
ggplot(data = sqf_data, aes(x = fct_rev(fct_infreq(suspect_race_description)), fill = factor(suspect_arrested_flag))) +
geom_bar() +
coord_flip() +
theme_minimal() +
xlab("Suspect Race") +
ylab("N Stops") +
scale_fill_brewer(type = "qual", palette = "Pastel1", name = "Suspect Arrested") +
labs(title = "Distribution of Arrest Status, By Suspect Race")
# arrests by race, stacked
ggplot(data = sqf_data, aes(x = fct_rev(fct_infreq(suspect_race_description)), fill = factor(suspect_arrested_flag))) +
geom_bar(position = "fill") +
coord_flip() +
scale_y_continuous(labels = scales::percent) +
theme_minimal() +
xlab("Suspect Race") +
ylab("% Stops") +
scale_fill_brewer(type = "qual", palette = "Pastel1", name = "Suspect Arrested") +
labs(title = "Distribution of Arrest Status, By Suspect Race")
# arrests by suspected crime description, unstacked
ggplot(data = sqf_data, aes(x = fct_rev(fct_infreq(suspected_crime_description)), fill = factor(suspect_arrested_flag))) +
geom_bar() +
coord_flip() +
theme_minimal() +
xlab("Suspected Crime") +
ylab("N Stops") +
scale_fill_brewer(type = "qual", palette = "Pastel1", name = "Suspect Arrested") +
labs(title = "Distribution of Arrest Status, By Suspected Crime")
# arrests by suspected crime description, stacked
ggplot(data = sqf_data, aes(x = suspected_crime_description, fill = factor(suspect_arrested_flag))) +
geom_bar(position = "fill") +
coord_flip() +
scale_y_continuous(labels = scales::percent) +
theme_minimal() +
xlab("Suspected Crime") +
ylab("% Stops") +
scale_fill_brewer(type = "qual", palette = "Pastel1", name = "Suspect Arrested") +
labs(title = "Distribution of Arrest Status, By Suspect Crime")
# heatmap of arrests by time of day and borough
ggplot(
sqf_data %>%
group_by(stop_location_boro_name, time_of_day) %>%
summarise(
percent_arrested = mean(suspect_arrested_flag, na.rm = TRUE) * 100,
.groups = "drop"
),
aes(
x = time_of_day,
y = stop_location_boro_name,
fill = percent_arrested
)
) +
geom_tile(color = "white") +
scale_fill_viridis_c(name = "Arrest Rate (%)") +
labs(
title = "Heat Map of Arrest Rates",
x = "Time of Day",
y = "Borough"
) +
theme_minimal()
# heatmap of arrests by issuing officer rank and suspect race
ggplot(
sqf_data %>%
group_by(issuing_officer_rank, suspect_race_description) %>%
summarise(
percent_arrested = mean(suspect_arrested_flag, na.rm = TRUE) * 100,
.groups = "drop"
),
aes(
x = issuing_officer_rank,
y = suspect_race_description,
fill = percent_arrested
)
) +
geom_tile(color = "white") +
scale_fill_viridis_c(name = "Arrest Rate (%)") +
labs(
title = "Heat Map of Arrest Rates",
x = "Officer in Uniform",
y = "Suspect Race"
) +
theme_minimal() +
theme(
axis.text.x = element_text(angle = 45, hjust = 1)
)
# drop 7 observations which have incorrect spatial info
sqf_data <- sqf_data %>%
filter(stop_location_x > 0)
dim(sqf_data)
# make spatial object for mapping
sqf_data_sf <- st_as_sf(sqf_data,
coords = c("stop_location_x", "stop_location_y"),
crs = 2263)  #  crs for New York (EPSG:2263)
# load in nta-level shapefile
remotes::install_github("mfherman/nycgeo")
library(nycgeo)
nyc_tract_shp <- nycgeo::nyc_boundaries(geography = "tract", add_acs_data = TRUE)
# check crs
st_crs(nyc_tract_shp)$epsg
# plot data onto shapefile by arrest status
ggplot() +
geom_sf(data = nyc_tract_shp, fill = "lightblue", color = "black", size = 0.3) +
geom_sf(data = sqf_data_sf, aes(color = as.factor(suspect_arrested_flag)), size = 0.7) +
scale_color_manual(values = c("red", "seagreen"),
labels = c("Arrested", "Not Arrested")) +
theme_minimal() +
labs(title = "NYC Police Stops by Arrest Status") +
theme(plot.title = element_text(hjust = 0.5),
plot.subtitle = element_text(hjust = 0.5),
legend.title = element_blank())
# join datasets to assign each stop to a tract
sqf_data_sf <- st_join(sqf_data_sf, nyc_tract_shp)
dim(sqf_data_sf)
# aggregate to tract level
sqf_data_sf_tract_level <- sqf_data_sf %>%
filter(!is.na(geoid)) %>%
group_by(geoid) %>%
summarize(pc_arrest = (sum(suspect_arrested_flag) / n()) * 100)
# join with shp for mapping
sqf_data_sf_tract_level <- nyc_tract_shp %>%
st_join(sqf_data_sf_tract_level, by = "geoid")
ggplot() +
geom_sf(data = sqf_data_sf_tract_level, aes(fill = pc_arrest), color = "black", size = 0.3) +
scale_fill_viridis_c(
name = "% Stops Ending in Arrest",
option = "inferno",
na.value = "white"
) +
theme_void() +
labs(title = "Percentage of Stops Ending in Arrest by NYC Census Tract")
# we first create the new variables in the shapefile for ease of plotting
nyc_tract_shp <- nyc_tract_shp %>%
mutate(pc_pop_black_est = (pop_black_est / pop_total_est) * 100,
pc_pop_hisp_est = (pop_hisp_est / pop_total_est) * 100,
pc_pop_asian_est = (pop_asian_est / pop_total_est) * 100,
pc_pop_ba_above_est = (pop_ba_above_est / pop_total_est) * 100,
pc_pop_inpov_est = (pop_inpov_est / pop_total_est) * 100
)
# non hispanic black
ggplot(nyc_tract_shp) +
geom_sf(aes(fill = pc_pop_black_est)) +
scale_fill_viridis_c(
name = "Non-Hispanic Black % Total Population",
option = "inferno"
) +
theme_void() +
labs(title = "Non-Hispanic Black Population by Census Tract, ACS 2013-2017")
# hispanic any
ggplot(nyc_tract_shp) +
geom_sf(aes(fill = pc_pop_hisp_est)) +
scale_fill_viridis_c(
name = "Hispanic Any Race % Total Population",
option = "inferno"
) +
theme_void() +
labs(title = "Hispanic Any Race Population by Census Tract, ACS 2013-2017")
# non
ggplot(nyc_tract_shp) +
geom_sf(aes(fill = pc_pop_asian_est)) +
scale_fill_viridis_c(
name = "Non-hispanic Asian % Total Population",
option = "inferno"
) +
theme_void() +
labs(title = "Non-hispanic Asian  Population by Census Tract, ACS 2013-2017")
# pop age 25 years or older with at least bachelors degree
ggplot(nyc_tract_shp) +
geom_sf(aes(fill = pc_pop_ba_above_est)) +
scale_fill_viridis_c(
name = "% Population Aged >= 25 with at Least Bachelors Degree",
option = "inferno"
) +
theme_void() +
labs(title = "% Population Aged >= 25 with at least a Bachelor's Degree by Census Tract, ACS 2013-2017")
# income below pov
# pop age 25 years or older with at least bachelors degree
ggplot(nyc_tract_shp) +
geom_sf(aes(fill = pc_pop_inpov_est)) +
scale_fill_viridis_c(
name = "% Population With Income Below Poverty Line",
option = "inferno"
) +
theme_void() +
labs(title = "% Population with Income Below Poverty Line, ACS 2013-2017")
# now we create the new variables in the sqf_data_sf object, before merging as appropriate by stop_id into the original data
sqf_data_sf <- sqf_data_sf %>%
mutate(pc_pop_black_est = (pop_black_est / pop_total_est) * 100,
pc_pop_hisp_est = (pop_hisp_est / pop_total_est) * 100,
pc_pop_asian_est = (pop_asian_est / pop_total_est) * 100,
pc_pop_ba_above_est = (pop_ba_above_est / pop_total_est) * 100,
pc_pop_inpov_est = (pop_inpov_est / pop_total_est) * 100
)
# check current dim
dim(sqf_data)
sqf_data <- sqf_data %>%
# left join selected spatial features from the sf object into sqf_data
left_join(sqf_data_sf %>% select(stop_id, pc_pop_ba_above_est, pc_pop_inpov_est, pc_pop_asian_est, pc_pop_hisp_est, pc_pop_black_est), by = "stop_id") %>%
# drop x,y coords and geometry as we use census tract for spatial info
select(-c("stop_location_x", "stop_location_y", "geometry")) %>%
# drop obs with missing values in these spatial features
filter(!if_any(everything(), is.na))
# check new dim
dim(sqf_data)
# set seed for reproducibility
set.seed(1)
# set y and X matrix
y <- sqf_data$suspect_arrested_flag
X <- model.matrix(~ . - suspect_arrested_flag - stop_id -search_basis_incidental_to_arrest_flag, data = sqf_data)
# perform train-test split
train_index <- createDataPartition(y, p = 0.7, list = FALSE)
y_train <- y[train_index]
y_test <- y[-train_index]
X_train <- X[train_index,]
X_test <- X[-train_index, ]
# print lengths and dimensions
cat("Length of y_train:", length(y_train), "\n")
cat("Length of y_test:", length(y_test), "\n")
cat("Dimensions of X_train:", dim(X_train), "\n")
cat("Dimensions of X_test:", dim(X_test), "\n")
# check balance of y
print(table(y_train))
print(table(y_test))
# set x subset, removing anything that is/might be realized during the stop
X_subset <- X[, !grepl("^(search|physical_force|.*eye_color|.*hair_color)", colnames(X)) &
!colnames(X) %in% c("frisked_flag", "firearm_flag", "knife_cutter_flag",
"other_weapon_flag", "weapon_found_flag", "other_contraband_flag")]
# perform train-test split
X_train_subset <- X_subset[train_index, ]
X_test_subset <- X_subset[-train_index, ]
cat("Dimensions of X_train_subset:", dim(X_train_subset), "\n")
cat("Dimensions of X_test_subset:", dim(X_test_subset), "\n")
# subset training data for validation split
n_train <- nrow(X_train)
id_split <- sample(1:n_train, floor(0.5 * n_train))
# split into training and validation sets
X_train_subset_final <- X_train_subset[id_split, ]
y_train_final <- y_train[id_split]
X_val_subset <- X_train_subset[-id_split, ]
y_val <- y_train[-id_split]
# print dimensions
cat("dimensions of X_train_final:", dim(X_train_subset_final), "\n")
cat("dimensions of y_train_final:", length(y_train_final), "\n")
cat("dimensions of X_val:", dim(X_val_subset), "\n")
cat("dimensions of y_val:", length(y_val), "\n")
# Set X_logit, removing unwanted columns based on the patterns
X_logit <- X_subset[, !grepl("^(supervising_officer_rank)", colnames(X_subset))]
# perform train-test split on X_logit
X_train_logit <- X_logit[train_index, ]
X_test_logit <- X_logit[-train_index, ]
# print lengths and dimensions
cat("Dimensions of X_train_logit:", dim(X_train_logit), "\n")
cat("Dimensions of X_test_subset:", dim(X_test_logit), "\n")
# glm requires dataframes as arguments
X_train_logit_df <- as.data.frame(X_train_logit)
X_test_logit_df <- as.data.frame(X_test_logit)
# full logit model in training data
logit <- glm(y_train ~ ., data = X_train_logit_df, family = binomial(logit))
# get fitted probabilities using trained model on test data
logit_predict <- as.numeric(predict(logit, newdata = X_test_logit_df, type = "response"))
# convert probabilities to class predictions using a threshold of 0.5
logit_y_hat <- ifelse(logit_predict > 0.5, 1, 0)
# inspect class balance of predicted classes
table(logit_y_hat)
# define a function to generate and plot confusion matrix
generate_cm <- function(true, predicted, title) {
# gen confusion matrix as a data frame
cm <- as.data.frame(table(True = true, Predicted = predicted)) %>%
group_by(Predicted) %>%
mutate(Predicted_pct = Freq / sum(Freq))
# print cm
print(cm)
# plot cm
plot <- ggplot(data = cm, mapping = aes(x = ordered(True, c(1, 0)), y = Predicted, fill = Predicted_pct)) +
geom_tile() +
geom_text(aes(label = round(Predicted_pct, 2)), color = 'white') +
scale_fill_gradient(low = "blue", high = "red", name = "Rel. Freq.") +
xlab("True") +
ylab("Predicted") +
labs(title = title) +
theme_minimal()
# print plot
print(plot)
}
# plot confusion matrix for logistic regression with all variables
generate_cm(y_test, logit_y_hat, "Confusion Matrix for Logistic Regression")
# compute ROC and AUC
logit_roc <- roc(response = y_test, predictor = logit_predict, quiet = TRUE)
logit_auc <- round(auc(logit_roc), 4)
cat("AUC for the logit model", logit_auc, "\n")
