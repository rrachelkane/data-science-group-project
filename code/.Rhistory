summarize(pc_arrest = (sum(suspect_arrested_flag) / n()) * 100)
# join with shp for mapping
sqf_data_sf_tract_level <- nyc_tract_shp %>%
st_join(sqf_data_sf_tract_level, by = "geoid")
ggplot() +
geom_sf(data = sqf_data_sf_tract_level, aes(fill = pc_arrest), color = "black", size = 0.3) +
scale_fill_viridis_c(
name = "% Stops Ending in Arrest",
option = "inferno",
na.value = "white"
) +
theme_void() +
labs(title = "Percentage of Stops Ending in Arrest by NYC Census Tract")
# non hispanic black
ggplot(nyc_tract_shp) +
geom_sf(aes(fill = pop_black_est)) +
scale_fill_viridis_c(
name = "Non-Hispanic Black Population",
option = "inferno"
) +
theme_void() +
labs(title = "Non-Hispanic Black Population by Census Tract, ACS 2013-2017")
# hispanic any
ggplot(nyc_tract_shp) +
geom_sf(aes(fill = pop_hisp_est)) +
scale_fill_viridis_c(
name = "Hispanic Any Race Population",
option = "inferno"
) +
theme_void() +
labs(title = "Hispanic Any Race Population by Census Tract, ACS 2013-2017")
# non
ggplot(nyc_tract_shp) +
geom_sf(aes(fill = pop_asian_est)) +
scale_fill_viridis_c(
name = "Non-hispanic Asian Population",
option = "inferno"
) +
theme_void() +
labs(title = "Non-hispanic Asian  Population by Census Tract, ACS 2013-2017")
# pop age 25 years or older with at least bachelors degree
ggplot(nyc_tract_shp) +
geom_sf(aes(fill = pop_ba_above_est)) +
scale_fill_viridis_c(
name = "Population Aged >= 25 with at Least Bachelors Degree",
option = "inferno"
) +
theme_void() +
labs(title = "Population Aged >= 25 with at least a Bachelor's Degree by Census Tract, ACS 2013-2017")
# income below pov
# pop age 25 years or older with at least bachelors degree
ggplot(nyc_tract_shp) +
geom_sf(aes(fill = pop_inpov_est)) +
scale_fill_viridis_c(
name = "Population With Income Below Poverty Line",
option = "inferno"
) +
theme_void() +
labs(title = "Population with Income Below Poverty Line, ACS 2013-2017")
# check current dim
dim(sqf_data)
sqf_data <- sqf_data %>%
# left join selected spatial features from the sf object into sqf_data
left_join(sqf_data_sf %>% select(stop_id, pop_ba_above_est, pop_inpov_est, pop_asian_est, pop_hisp_est, pop_black_est), by = "stop_id") %>%
# drop x,y coords and geometry as we use census tract for spatial info
select(-c("stop_location_x", "stop_location_y", "geometry")) %>%
# drop obs with missing values in these spatial features
filter(!if_any(everything(), is.na))
# check new dim
dim(sqf_data)
# set seed for reproducibility
set.seed(1)
# set y and X matrix
y <- sqf_data$suspect_arrested_flag
X <- model.matrix(~ . - suspect_arrested_flag - stop_id -search_basis_incidental_to_arrest_flag, data = sqf_data)
# perform train-test split
train_index <- createDataPartition(y, p = 0.7, list = FALSE)
y_train <- y[train_index]
y_test <- y[-train_index]
X_train <- X[train_index,]
X_test <- X[-train_index, ]
# print lengths and dimensions
cat("Length of y_train:", length(y_train), "\n")
cat("Length of y_test:", length(y_test), "\n")
cat("Dimensions of X_train:", dim(X_train), "\n")
cat("Dimensions of X_test:", dim(X_test), "\n")
# check balance of y
print(table(y_train))
print(table(y_test))
# set x subset, removing anything that is/might be realized during the stop
X_subset <- X[, !grepl("^(search|physical_force|.*eye_color)", colnames(X)) &
!colnames(X) %in% c("frisked_flag", "firearm_flag", "knife_cutter_flag",
"other_weapon_flag", "weapon_found_flag", "other_contraband_flag")]
# perform train-test split
X_train_subset <- X_subset[train_index, ]
X_test_subset <- X_subset[-train_index, ]
cat("Dimensions of X_train_subset:", dim(X_train_subset), "\n")
cat("Dimensions of X_test_subset:", dim(X_test_subset), "\n")
# subset training data for validation split
n_train <- nrow(X_train)
id_split <- sample(1:n_train, floor(0.5 * n_train))
# split into training and validation sets
X_train_subset_final <- X_train_subset[id_split, ]
y_train_final <- y_train[id_split]
X_val_subset <- X_train_subset[-id_split, ]
y_val <- y_train[-id_split]
# print dimensions
cat("dimensions of X_train_final:", dim(X_train_subset_final), "\n")
cat("dimensions of y_train_final:", length(y_train_final), "\n")
cat("dimensions of X_val:", dim(X_val_subset), "\n")
cat("dimensions of y_val:", length(y_val), "\n")
# Set X_logit, removing unwanted columns based on the patterns
X_logit <- X[, !grepl("^(suspected_crime_description|frisked_flag|firearm_flag|knife_cutter_flag|other_weapon_flag|weapon_found_flag|other_contraband_flag|physical_force|search_basis|suspect_eye_color|suspect_hair_color|supervising_officer_rank)", colnames(X))]
# perform train-test split on X_logit
X_train_logit <- X_logit[train_index, ]
X_test_logit <- X_logit[-train_index, ]
# print lengths and dimensions
cat("Dimensions of X_train_logit:", dim(X_train_logit), "\n")
cat("Dimensions of X_test_subset:", dim(X_test_logit), "\n")
# glm requires dataframes as arguments
X_train_logit_df <- as.data.frame(X_train_logit)
X_test_logit_df <- as.data.frame(X_test_logit)
# full logit model in training data
logit <- glm(y_train ~ ., data = X_train_logit_df, family = binomial(logit))
# get fitted probabilities using trained model on test data
logit_predict <- as.numeric(predict(logit, newdata = X_test_logit_df, type = "response"))
# convert probabilities to class predictions using a threshold of 0.5
logit_y_hat <- ifelse(logit_predict > 0.5, 1, 0)
# inspect class balance of predicted classes
table(logit_y_hat)
# define a function to generate and plot confusion matrix
generate_cm <- function(true, predicted, title) {
# gen confusion matrix as a data frame
cm <- as.data.frame(table(True = true, Predicted = predicted)) %>%
group_by(Predicted) %>%
mutate(Predicted_pct = Freq / sum(Freq))
# print cm
print(cm)
# plot cm
plot <- ggplot(data = cm, mapping = aes(x = ordered(True, c(1, 0)), y = Predicted, fill = Predicted_pct)) +
geom_tile() +
geom_text(aes(label = round(Predicted_pct, 2)), color = 'white') +
scale_fill_gradient(low = "blue", high = "red", name = "Rel. Freq.") +
xlab("True") +
ylab("Predicted") +
labs(title = title) +
theme_minimal()
# print plot
print(plot)
}
# plot confusion matrix for logistic regression with all variables
generate_cm(y_test, logit_y_hat, "Confusion Matrix for Logistic Regression")
# compute ROC and AUC
logit_roc <- roc(response = y_test, predictor = logit_predict, quiet = TRUE)
logit_auc <- round(auc(logit_roc), 4)
cat("AUC for the logit model", logit_auc, "\n")
# run lasso on training data to collect coefficients
lasso <- cv.glmnet(x=X_train, y=y_train, alpha = 1, family="binomial", type.measure = "class")
# optimal lambda
lasso_lambda_min <- lasso$lambda.min
cat("Optimal Lambda for Lasso:", lasso_lambda_min, "\n")
# plot misclassification error against log lambda
plot(lasso)
abline(v = log(lasso_lambda_min), col = "red", lwd = 2, lty = 2)
title(main = "Cross-Validation Misclassification Error (LASSO - Full)",
sub = "Optimal Lambda Highlighted in Red",
xlab = "Log(Lambda)", ylab = "Misclassification Error")
# plot coefficients
plot(lasso$glmnet.fit, xvar = "lambda", label = TRUE)
title(main = "Coefficient Shrinkage Path for LASSO (Full Model)",
xlab = "Log(Lambda)", ylab = "Coefficients")
# get fitted probabilities, using best lambda
lasso_predict <- as.numeric(predict(lasso, s = lasso_lambda_min, X_test, type = "response"))
# add variable for predicted classes
lasso_y_hat <- ifelse(lasso_predict > 0.5, 1, 0)
# plot confusion matrix for lasso regression out of sample
generate_cm(y_test, lasso_y_hat, "Confusion Matrix for LASSO (Full Model)")
# compute ROC and AUC
lasso_roc_full <- roc(response = y_test, predictor = lasso_predict, quiet = TRUE)
lasso_auc_full <- round(auc(lasso_roc_full), 4)
cat("AUC for the LASSO (full) model", lasso_auc_full, "\n")
# run lasso on training data to collect coefficients
lasso_subset <- cv.glmnet(x=X_train_subset, y=y_train, alpha = 1, family="binomial", type.measure = "class")
lasso_lambda_min_subset <- lasso_subset$lambda.min
# plot misclassification error against log lambda
plot(lasso_subset)
abline(v = log(lasso_lambda_min_subset), col = "red", lwd = 2, lty = 2)
title(main = "Cross-Validation Misclassification Error (LASSO - Subset)",
sub = "Optimal Lambda Highlighted in Red",
xlab = "Log(Lambda)", ylab = "Misclassification Error")
# plot coefficients
plot(lasso_subset$glmnet.fit, xvar = "lambda", label = TRUE)
title(main = "Coefficient Shrinkage Path for LASSO (Subset)",
xlab = "Log(Lambda)", ylab = "Coefficients")
# get fitted probabilities
lasso_predict_subset <- as.numeric(predict(lasso_subset, s = lasso_lambda_min_subset, X_test_subset, type = "response"))
# add variable for predicted classes
lasso_y_hat_subset <- ifelse(lasso_predict_subset > 0.5, 1, 0)
# plot confusion matrix for lasso subset regression out of sample
generate_cm(y_test, lasso_y_hat_subset, "Confusion Matrix for LASSO (Subset Model)")
# compute roc object for subset model
lasso_roc_subset <- roc(response = y_test, predictor = lasso_predict_subset, quiet = TRUE)
lasso_auc_subset <- round(auc(lasso_roc_subset), 4)
cat("AUC for the LASSO (subset) model", lasso_auc_subset, "\n")
# run ridge regression on training data to collect coefficients
ridge <- cv.glmnet(x = X_train, y = y_train, alpha = 0, family = "binomial", type.measure = "class")
ridge_lambda_min <- ridge$lambda.min
cat("optimal lambda for ridge:", ridge_lambda_min, "\n")
# plot misclassification error against log lambda
plot(ridge)
abline(v = log(ridge_lambda_min), col = "red", lwd = 2, lty = 2)
title(main = "Cross-Validation Misclassification Error (Ridge - Full)",
sub = "Optimal Lambda Highlighted in Red",
xlab = "Log(Lambda)", ylab = "Misclassification Error")
# plot coefficients
plot(ridge$glmnet.fit, xvar = "lambda", label = TRUE)
title(main = "Coefficient Shrinkage Path for Ridge (Full Model)",
xlab = "Log(Lambda)", ylab = "Coefficients")
# get fitted probabilities
ridge_predict <- as.numeric(predict(ridge, s = ridge_lambda_min, X_test, type = "response"))
# add variable for predicted classes
ridge_y_hat <- ifelse(ridge_predict > 0.5, 1, 0)
# plot confusion matrix for ridge regression out of sample
generate_cm(y_test, ridge_y_hat, "Confusion Matrix for Ridge (Full Model)")
# compute roc object for full ridge model
ridge_roc_full <- roc(response = y_test, predictor = ridge_predict, quiet = TRUE)
ridge_auc_full <- round(auc(ridge_roc_full), 4)
cat("AUC for the Ridge (full) model:", ridge_auc_full, "\n")
# run ridge regression on subset predictors
ridge_subset <- cv.glmnet(x = X_train_subset, y = y_train, alpha = 0, family = "binomial", type.measure = "class")
ridge_lambda_min_subset <- ridge_subset$lambda.min
cat("Optimal lambda for ridge (subset):", ridge_lambda_min_subset, "\n")
# plot misclassification error against log lambda
plot(ridge_subset)
abline(v = log(ridge_lambda_min_subset), col = "red", lwd = 2, lty = 2)
title(main = "Cross-Validation Misclassification Error (Ridge - Subset)",
sub = "Optimal Lambda Highlighted in Red",
xlab = "Log(Lambda)", ylab = "Misclassification Error")
# plot coefficients
plot(ridge_subset$glmnet.fit, xvar = "lambda", label = TRUE)
title(main = "Coefficient Shrinkage Path for Ridge (Subset)",
xlab = "Log(Lambda)", ylab = "Coefficients")
# get fitted probabilities
ridge_predict_subset <- as.numeric(predict(ridge_subset, s = ridge_lambda_min_subset, X_test_subset, type = "response"))
# add variable for predicted classes
ridge_y_hat_subset <- ifelse(ridge_predict_subset > 0.5, 1, 0)
# plot confusion matrix for ridge subset regression out of sample
generate_cm(y_test, ridge_y_hat_subset, "Confusion Matrix for Ridge (Subset Model)")
# compute roc object for subset ridge model
ridge_roc_subset <- roc(response = y_test, predictor = ridge_predict_subset, quiet = TRUE)
ridge_auc_subset <- round(auc(ridge_roc_subset), 4)
cat("AUC for the Ridge (subset) model:", ridge_auc_subset, "\n")
# Define alpha values for grid search
alpha_values <- seq(0, 1, by = 0.1)
# Initialize storage for results
results <- data.frame(alpha = alpha_values, auc = NA, optimal_lambda = NA)
# loop grid search
for (i in 1:nrow(results)) {
alpha_i <- results$alpha[i]
# train en model with cross-validation for the given alpha
en_opt <- cv.glmnet(
x = X_train_subset_final,
y = y_train_final,
alpha = alpha_i,
family = "binomial",
type.measure = "class"
)
# get the best lambda for the current alpha
en_opt_lambda_min <- en_opt$lambda.min
# get fitted probabilities on the validation set using the best lambda and current alpha
en_opt_predict_val <- as.numeric(predict(en_opt, s = en_opt_lambda_min, newx = X_val_subset, type = "response"))
# compute roc and aucs of trained model in validation data
en_opt_roc_val <- roc(response = as.numeric(y_val), predictor = en_opt_predict_val, quiet = TRUE)
en_opt_auc_val <- auc(en_opt_roc_val)
# add to storage vector
results$auc[i] <- en_opt_auc_val
results$optimal_lambda[i] <- en_opt_lambda_min
}
# identify the best alpha
best_params <- results[which.max(results$auc), ]
cat("Optimal alpha:", best_params$alpha, "\n")
cat("Optimal lambda:", best_params$optimal_lambda, "\n")
cat("Best AUC:", best_params$auc, "\n")
# train the tuned elastic net model with best hyperparameters
en_opt <- glmnet(
x = X_train_subset_final,
y = y_train_final,
alpha = best_params$alpha,
lambda = best_params$optimal_lambda,
family = "binomial"
)
# get fitted probabilities using trained model on test data
en_opt_predict <- as.numeric(predict(en_opt, s = best_params$optimal_lambda, newx = X_test_subset, type = "response"))
# add variable for predicted classes
en_opt_y_hat <- ifelse(en_opt_predict > 0.5, 1, 0)
# plot confusion matrix for tuned elastic net regression out of sample
generate_cm(y_test, en_opt_y_hat, "Confusion Matrix for Elastic Net (Subset Model)")
# compute test ROC and AUC out of sample
en_opt_roc_subset <- roc(response = as.numeric(y_test), predictor = en_opt_predict, quiet = TRUE)
en_opt_auc_subset <- auc(en_opt_roc_subset)
cat("Test AUC for Tuned Elastic Net model:", en_opt_auc_subset, "\n")
# Initialize parameter grid for grid search
ntrees <- seq(50, 150, 50)  # range of trees to tune over
max_features_range <- 1:20  # Range of mtry  (max features) to tune over
# initialize storage for accuracy metrics
results <- expand.grid(ntree = ntrees, mtry = max_features_range)
dim(results)
results$train_acc <- NA
results$test_acc <- NA
results$oob_acc <- NA
# tune
for (i in 1:nrow(results)) {
ntrees_i <- results$ntree[i]
mtry_i <- results$mtry[i]
# train rf model using training data and current iteration of mtry and ntree
rf <- randomForest(
x = X_train_subset_final,
y = factor(y_train_final),
mtry = mtry_i,
ntree = ntrees_i
)
# use trained model to predict y in validation data
y_pred_val <- predict(rf, X_val_subset)
# get trained model predictions of y in training data
y_pred_train <- rf$predicted
# compute out of bag accuracy measure
results$oob_acc[i] <- 1 - rf$err.rate[ntrees_i, "OOB"]
# compute training accuracy measure
results$train_acc[i] <- mean(y_train_final == as.numeric(levels(y_pred_train)[y_pred_train]))
# compute "test" - validation - accuracy
results$test_acc[i] <- mean(y_val == as.numeric(levels(y_pred_val)[y_pred_val]))
}
# identify the optimal parameters
best_params <- results[which.max(results$test_acc), ]
cat("Optimal ntree:", best_params$ntree, "\n")
cat("Optimal mtry:", best_params$mtry, "\n")
# define accuracy metrics and y-axis range for tuning plot
accuracy_metrics <- results[results$ntree == best_params$ntree, ]
y_range <- range(c(accuracy_metrics$train_acc, accuracy_metrics$test_acc, accuracy_metrics$oob_acc)) + c(-0.01, 0.01)
# plot train, validation, and OOB accuracy for optimal ntree as mtry varies
plot(
accuracy_metrics$mtry, accuracy_metrics$train_acc,
type = 'l', col = 'blue', ylim = y_range,
main = 'Tuning mtry for Optimal ntree',
xlab = 'Number of Features (mtry)', ylab = 'Accuracy'
)
lines(accuracy_metrics$mtry, accuracy_metrics$test_acc, col = 'red')
lines(accuracy_metrics$mtry, accuracy_metrics$oob_acc, col = 'green', lty = 2)
legend('bottomright',
legend = c('Train Accuracy', 'Validation Accuracy', 'OOB Accuracy'),
col = c('blue', 'red', 'green'), lty = c(1, 1, 2), bty = 'n')
# given optimal parameters, train using training data to get tuned model
RFTuned <- randomForest(
x = X_train_subset_final,
y = factor(y_train_final),
mtry = best_params$mtry,
ntree = best_params$ntree
)
# using model trained with optimal parameters, predict with TEST data
RFPred <- predict(RFTuned, newdata = X_test_subset)
# compute associated relevant fitted probabilities
RFProb <- predict(RFTuned, newdata = X_test_subset, type = "prob")
# compute test roc and auc of tuned model
RF_roc <- roc(response = factor(y_test), predictor = RFProb[, 2], levels = c(0, 1), quiet = TRUE)
RF_auc <- round(auc(RF_roc), 4)
cat("Random Forest AUC (Subset):", RF_auc, "\n")
# gen confusion matrix
generate_cm(
true = y_test,
predicted = RFPred,
title = "Confusion Matrix for Random Forest (Subset)"
)
# function to plot ROC curves for multiple models
plot_multiple_roc <- function(roc_list, labels, colors, lwd_list, lty_list, main_title) {
# Plot the first model as the base plot
plot(
x = 1 - roc_list[[1]]$specificities,
y = roc_list[[1]]$sensitivities,
type = "l",
col = colors[1],
lwd = lwd_list[1],
lty = lty_list[1],
xlim = c(0, 1),
ylim = c(0, 1),
main = main_title,
xlab = "False Positive Rate",
ylab = "True Positive Rate",
cex.lab = 1.5,
cex.main = 1.8,
cex.axis = 1.2
)
# Add the remaining ROC curves
for (i in 2:length(roc_list)) {
lines(
x = 1 - roc_list[[i]]$specificities,
y = roc_list[[i]]$sensitivities,
col = colors[i],
lwd = lwd_list[i],
lty = lty_list[i]
)
}
# Add legend
legend(
"bottomright",
legend = labels,
col = colors,
lwd = lwd_list,
lty = lty_list,
bty = "n"
)
}
# set list of input rcs and associated labels, colors etc for arguments
roc_list <- list(logit_roc, lasso_roc_full, ridge_roc_full, RF_roc, ridge_roc_subset, lasso_roc_subset, en_opt_roc_subset)
labels <- c(
sprintf("LASSO Full (AUC: %.4f)", lasso_auc_full),
sprintf("Ridge Full (AUC: %.4f)", ridge_auc_full),
sprintf("Logistic Regression (AUC: %.4f)", logit_auc),
sprintf("Random Forest Subset (AUC: %.4f)", RF_auc),
sprintf("Ridge Subset (AUC: %.4f)", ridge_auc_subset),
sprintf("LASSO Subset (AUC: %.4f)", lasso_auc_subset),
sprintf("Elastic Net Subset (AUC: %.4f)", en_opt_auc_subset)
)
colors <- c("red", "darkgreen", "blue", "darkorange", "lightgreen", "pink", "purple")
lwd_list <- c(3, 3, 2, 2, 2, 2, 2)
lty_list <- c(2, 3, 1, 1, 3, 2, 1)
#call
plot_multiple_roc(
roc_list = roc_list,
labels = labels,
colors = colors,
lwd_list = lwd_list,
lty_list = lty_list,
main_title = "ROC Curves for All Models"
)
# make and print sorted table of AUCs
auc_table <- data.frame(
Model = c(
"Logistic Regression",
"LASSO Full",
"LASSO Subset",
"Ridge Full",
"Ridge Subset",
"Elastic Net Subset",
"Random Forest Subset"
),
AUC = c(
logit_auc,
lasso_auc_full,
lasso_auc_subset,
ridge_auc_full,
ridge_auc_subset,
en_opt_auc_subset,
RF_auc
)
)
auc_table <- auc_table %>%
arrange(desc(AUC))
print(auc_table)
# Function to compute and plot variable importance
compute_variable_importance <- function(coefficients, model_name, top_n = 20) {
importance_df <- data.frame(
Variable = names(coefficients),
Scaled_Importance = abs(coefficients) / max(abs(coefficients), na.rm = TRUE)
) %>%
filter(Scaled_Importance > 0) %>%
arrange(desc(Scaled_Importance)) %>%
slice(1:top_n)
ggplot(importance_df, aes(x = reorder(Variable, Scaled_Importance), y = Scaled_Importance)) +
geom_bar(stat = "identity", fill = "blue", alpha = 0.7) +
coord_flip() +
labs(
title = paste("Top", top_n, "Var Imp for", model_name),
x = "Variable",
y = "Scaled Importance"
) +
theme_minimal()
}
# 1. Logistic Regression
logit_coeff <- coef(logit)
logit_coeff <- logit_coeff[-1]
logit_plot <- compute_variable_importance(logit_coeff, "Logistic Regression")
print(logit_plot)
# 2. LASSO Subset
lasso_subset_coeff <- as.numeric(coef(lasso_subset, s = lasso_lambda_min_subset))
names(lasso_subset_coeff) <- rownames(coef(lasso_subset, s = lasso_lambda_min_subset))
lasso_subset_coeff <- lasso_subset_coeff[-1]
lasso_plot <- compute_variable_importance(lasso_subset_coeff, "LASSO Subset")
print(lasso_plot)
# 3. Ridge Subset
ridge_subset_coeff <- as.numeric(coef(ridge_subset, s = ridge_lambda_min_subset))
names(ridge_subset_coeff) <- rownames(coef(ridge_subset, s = ridge_lambda_min_subset))
ridge_subset_coeff <- ridge_subset_coeff[-1]
ridge_plot <- compute_variable_importance(ridge_subset_coeff, "Ridge Subset")
print(ridge_plot)
# Extract Random Forest importance
rf_importance_subset <- data.frame(
Variable = rownames(RFTuned$importance),
Scaled_Importance = RFTuned$importance[, "MeanDecreaseGini"] / max(RFTuned$importance[, "MeanDecreaseGini"], na.rm = TRUE)
)
rf_importance_subset <- rf_importance_subset %>%
arrange(desc(Scaled_Importance))
# plot rf variable importance - top 20
ggplot(rf_importance_subset %>% arrange(desc(Scaled_Importance)) %>% slice(1:20),
aes(x = reorder(Variable, Scaled_Importance), y = Scaled_Importance)) +
geom_bar(stat = "identity", fill = "blue", alpha = 0.7) +
coord_flip() +
labs(title = "Top 20 Var Imp for RF",
x = "Variable",
y = "Scaled Importance") +
theme_minimal()
View(sqf_data)
View(sqf_data_sf)
