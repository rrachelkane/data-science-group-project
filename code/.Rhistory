na_cols_df <- data.frame(Feature = names(na_cols), Percentage = na_cols)
# order for plot
na_cols_df$Feature <- factor(na_cols_df$Feature,
levels = na_cols_df$Feature[order(na_cols_df$Percentage, decreasing = FALSE)])
# plot
ggplot(na_cols_df, aes(x = Feature, y = Percentage)) +
geom_bar(stat = "identity", fill = "#F8566D", color = "black") +
labs(title = "Percentage of NA Values per Column",
x = "Columns",
y = "Percentage of NA Values") +
coord_flip() +  # Flip coordinates
theme_minimal()
sqf_data <- sqf_data %>%
select(-all_of(names(na_cols[na_cols > 25])))
dim(sqf_data)
sqf_data <- sqf_data %>%
filter(!if_any(everything(), is.na))
dim(sqf_data)
# pre check
print(unique(sqf_data$frisked_flag))
# clean Ys and Ns and set as numeric
sqf_data <- sqf_data %>%
mutate(across(
where(~ all(. %in% c("Y", "N"))),
~ as.numeric(ifelse(. == "Y", 1, ifelse(. == "N", 0, NA)))  #
))
# post check
print(unique(sqf_data$frisked_flag))
sqf_data <- sqf_data %>%
mutate(
time_of_day = case_when(
str_extract(stop_frisk_time, "^\\d{2}") %in% c("06", "07", "08", "09", "10", "11") ~ "Morning",
str_extract(stop_frisk_time, "^\\d{2}") %in% c("12", "13", "14", "15", "16", "17") ~ "Afternoon",
str_extract(stop_frisk_time, "^\\d{2}") %in% c("18", "19", "20", "21", "22", "23") ~ "Evening",
str_extract(stop_frisk_time, "^\\d{2}") %in% c("00", "01", "02", "03", "04", "05") ~ "Night",
TRUE ~ NA_character_  # Handle unexpected or missing values
),
time_of_day = factor(time_of_day, levels = c("Morning", "Afternoon", "Evening", "Night"))  # Ensure factor order
)
# Verify the result
print(table(sqf_data$time_of_day))
# drop stop frisk time as we will just use time_of_day
sqf_data <- sqf_data %>%
select(-"stop_frisk_time")
# convert character columns to factors, except for stop location x and y
sqf_data <- sqf_data %>%
mutate(across(
.cols = where(is.character) & !c("stop_location_x", "stop_location_y"),
.fns = as.factor
))
library(dplyr)
# Function to safely convert factor to numeric, handling non-numeric entries
convert_to_numeric <- function(x) {
# Convert to character to avoid factor levels issues
x <- as.character(x)
# Replace non-numeric values with NA (e.g., "unknown", "760", "7.6")
x <- gsub("[^0-9.]", "", x)
# Convert to numeric
as.numeric(x)
}
# Apply the function to the relevant columns
sqf_data <- sqf_data %>%
mutate(
suspect_reported_age = convert_to_numeric(suspect_reported_age),
suspect_height = convert_to_numeric(suspect_height),
suspect_weight = convert_to_numeric(suspect_weight)
)
#Check to make sure successful
summary(sqf_data$suspect_reported_age)
summary(sqf_data$suspect_height)
summary(sqf_data$suspect_weight)
# Compute density for reported_age
reported_age_density <- density(sqf_data$suspect_reported_age, na.rm = TRUE)
# Plot the density
plot(reported_age_density,
main = "Density of Reported Age with Outliers Highlighted",
xlab = "Reported Age",
ylab = "Density",
col = "black",
lwd = 2)
grid()
# Identify and highlight outliers (ages < 10 and > 85)
outliers <- sqf_data$suspect_reported_age[sqf_data$suspect_reported_age < 10 | sqf_data$suspect_reported_age > 85]
# Add vertical lines to mark the outlier boundaries
abline(v = c(10, 85), col = "deeppink", lty = 2, lwd = 2)
# Highlight the outlier data points on the plot
points(outliers, rep(0, length(outliers)), col = "deeppink", pch = 19)
#We will drop outlier observations where age is above 85 and below 10.
sqf_data <- sqf_data[sqf_data$suspect_reported_age >= 10 & sqf_data$suspect_reported_age <= 85, ]
# Compute density for suspect_height
height_density <- density(sqf_data$suspect_height, na.rm = TRUE)
# Plot the density
plot(height_density,
main = "Density of Height with Outliers Highlighted",
xlab = "Height",
ylab = "Density",
col = "black",
lwd = 2)
grid()
# Identify and highlight outliers (e.g., heights below 4 feet and above 7 feet)
outliers <- sqf_data$suspect_height[sqf_data$suspect_height < 4 | sqf_data$suspect_height > 7]
# Add vertical lines to mark the outlier boundaries
abline(v = c(4, 7), col = "darkorchid2", lty = 2, lwd = 2)
# Highlight the outlier data points on the plot
points(outliers, rep(0, length(outliers)), col = "darkorchid2", pch = 19)
#We will drop outlier observations where height is above 7 ft and below 4 ft.
sqf_data <- sqf_data[sqf_data$suspect_height >= 4 & sqf_data$suspect_height <= 7, ]
# Compute density for suspect_weight
weight_density <- density(sqf_data$suspect_weight, na.rm = TRUE)
# Plot the density
plot(weight_density,
main = "Density of Suspect Weight with Outliers Highlighted",
xlab = "Weight",
ylab = "Density",
col = "black",
lwd = 2)
grid()
# Identify and highlight outliers (e.g., weights below 90 lbs and above 300 lbs)
outliers <- sqf_data$suspect_weight[sqf_data$suspect_weight < 90 | sqf_data$suspect_weight > 300]
# Add vertical lines to mark the outlier boundaries
abline(v = c(90, 300), col = "darkturquoise", lty = 2, lwd = 2)
# Highlight the outlier data points on the plot
points(outliers, rep(0, length(outliers)), col = "darkturquoise", pch = 19)
#We will drop outlier observations where height is above 300 lbs. and below 90 lbs.
sqf_data <- sqf_data[sqf_data$suspect_weight >= 90 & sqf_data$suspect_weight <= 300, ]
# Standardize the numeric variables
sqf_data <- sqf_data %>%
mutate(
suspect_reported_age = scale(suspect_reported_age),
suspect_height = scale(suspect_height),
suspect_weight = scale(suspect_weight)
)
# Check if the standardization worked by summarizing the variables
summary(sqf_data$suspect_reported_age)
summary(sqf_data$suspect_height)
summary(sqf_data$suspect_weight)
# check dim again
dim(sqf_data)
# tabulation of the dependent variable
sqf_data %>%
group_by(suspect_arrested_flag) %>%
summarise(N = n(),
Pc = N / nrow(sqf_data) * 100) %>%
arrange(desc(N)) %>%
kable(booktabs = TRUE, col.names = c("Suspect Arrested", "N Stops", "% Total Stops"), align = "l")
# looking at the distribution of sex
ggplot(sqf_data, aes(x = suspect_sex, fill = suspect_sex)) +
geom_bar() +
labs(
title = "Distribution of Suspect Sex",
x = "Sex",
y = "Count"
) +
theme_minimal() +
scale_fill_manual(
values = c("MALE" = "lightblue", "FEMALE" = "pink")
) +
theme(legend.position = "none")
# sex by arrest status
ggplot(sqf_data, aes(x = suspect_sex, fill = factor(suspect_arrested_flag))) +
geom_bar(position = "fill") +
labs(title = "Distribution of Suspect Sex",
x = "Sex",
y = "Count") +
theme_minimal() +
scale_y_continuous(labels = scales::percent) +
scale_fill_brewer(type = "qual", palette = "Pastel2", name = "Suspect Arrested")
# empirical cdf of age by sex and arrest status
ggplot(sqf_data, aes(x = suspect_reported_age, color = factor(suspect_arrested_flag))) +
stat_ecdf(geom = "step") +
facet_wrap(~ suspect_sex, ncol = 2) +
scale_color_manual(values = c("0" = "red", "1" = "darkgreen"),
labels = c("Not Arrested", "Arrested"),
name = "Arrest Outcome") +
labs(x = "Suspect Reported Age", y = "ECDF", title = "Empirical CDF of Suspect Reported Age, By Sex and Arrest Status") +
theme_minimal()
# may need cleaning
arrests_by_age_sex <- sqf_data %>%
mutate(
age_bucket = cut(
suspect_reported_age,
breaks = seq(0, 80, by = 15),
labels = paste(seq(0, 65, by = 15), seq(15, 80, by = 15), sep = "-"),
include.lowest = TRUE
),
arrested_flag = factor(ifelse(suspect_arrested_flag == 1, "Arrested", "Not Arrested"), levels = c("Arrested", "Not Arrested"))
) %>%
group_by(age_bucket, suspect_sex, arrested_flag) %>%
summarise(count = sum(!is.na(suspect_arrested_flag)), .groups = "drop") %>% # Correct counting
group_by(age_bucket, suspect_sex) %>%
mutate(percentage = 100 * count / sum(count)) %>%
ungroup()
# plot
ggplot(arrests_by_age_sex, aes(x = age_bucket, y = percentage, fill = arrested_flag)) +
geom_col(alpha = 0.8, position = "stack") +
facet_wrap(~ suspect_sex, ncol = 2) +
scale_fill_brewer(palette = "Pastel1", name = "Outcome") + # edit colour ordering here
labs(x = "Age Bucket", y = "% of Individuals", fill = "Outcome") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) # needs to be edited for NA
# distribution of race
ggplot(sqf_data, aes(x = suspect_race_description, fill = factor(suspect_race_description))) +
geom_bar() +
labs(
title = "Distribution of Suspect Race",
x = "Race",  # Updated x-axis label
y = "Count"
) +
theme_minimal() +
scale_fill_brewer(type = "qual", palette = "Pastel1", name = "Suspect Arrested") +
theme(
axis.text.x = element_text(angle = 45, hjust = 1),
legend.position = "none"
)
# arrests by race, unstacked
ggplot(data = sqf_data, aes(x = fct_rev(fct_infreq(suspect_race_description)), fill = factor(suspect_arrested_flag))) +
geom_bar() +
coord_flip() +
theme_minimal() +
xlab("Suspect Race") +
ylab("N Observations") +
scale_fill_brewer(type = "qual", palette = "Pastel1", name = "Suspect Arrested") +
labs(title = "Suspect Arrested, By Race")
# arrests by race, stacked
ggplot(data = sqf_data, aes(x = fct_rev(fct_infreq(suspect_race_description)), fill = factor(suspect_arrested_flag))) +
geom_bar(position = "fill") +
coord_flip() +
scale_y_continuous(labels = scales::percent) +
theme_minimal() +
xlab("Suspect Race") +
ylab("% Observations") +
scale_fill_brewer(type = "qual", palette = "Pastel1", name = "Suspect Arrested") +
labs(title = "Suspect Arrested, By Race")
# arrests by suspected crime description, unstacked
ggplot(data = sqf_data, aes(x = fct_rev(fct_infreq(suspected_crime_description)), fill = factor(suspect_arrested_flag))) +
geom_bar() +
coord_flip() +
theme_minimal() +
xlab("Suspected Crime") +
ylab("N Observations") +
scale_fill_brewer(type = "qual", palette = "Pastel1", name = "Suspect Arrested") +
labs(title = "Suspect Arrested, By Suspected Crime")
# arrests by suspected crime description, stacked
ggplot(data = sqf_data, aes(x = suspected_crime_description, fill = factor(suspect_arrested_flag))) +
geom_bar(position = "fill") +
coord_flip() +
scale_y_continuous(labels = scales::percent) +
theme_minimal() +
xlab("Suspected Crime") +
ylab("% Observations") +
scale_fill_brewer(type = "qual", palette = "Pastel1", name = "Suspect Arrested") +
labs(title = "Suspect Arrested, By Suspected Crime")
# to be done:
# time of stop
# cop side stuff
# any other variables of importance
# cormat/heatmaps/PCA
# look for outliers here!
# generate features as needed
# drop 7 observations which have incorrect spatial info
sqf_data <- sqf_data %>%
filter(stop_location_x > 0)
dim(sqf_data)
# make spatial object for mapping
sqf_data_sf <- st_as_sf(sqf_data,
coords = c("stop_location_x", "stop_location_y"),
crs = 2263)  #  crs for New York (EPSG:2263)
# load in nta-level shapefile
remotes::install_github("mfherman/nycgeo")
library(nycgeo)
nyc_tract_shp <- nycgeo::nyc_boundaries(geography = "tract", add_acs_data = TRUE)
# check crs
st_crs(nyc_tract_shp)$epsg
# plot data onto shapefile by arrest status
ggplot() +
geom_sf(data = nyc_tract_shp, fill = "lightblue", color = "black", size = 0.3) +
geom_sf(data = sqf_data_sf, aes(color = as.factor(suspect_arrested_flag)), size = 0.7) +
scale_color_manual(values = c("red", "seagreen"),
labels = c("Arrested", "Not Arrested")) +
theme_minimal() +
labs(title = "NYC Police Stops by Arrest Status") +
theme(plot.title = element_text(hjust = 0.5),
plot.subtitle = element_text(hjust = 0.5),
legend.title = element_blank())
# join datasets to assign each stop to a tract
sqf_data_sf <- st_join(sqf_data_sf, nyc_tract_shp)
dim(sqf_data_sf)
# aggregate to tract level
sqf_data_sf_tract_level <- sqf_data_sf %>%
filter(!is.na(geoid)) %>%
group_by(geoid) %>%
summarize(pc_arrest = (sum(suspect_arrested_flag) / n()) * 100)
# join with shp for mapping
sqf_data_sf_tract_level <- nyc_tract_shp %>%
st_join(sqf_data_sf_tract_level, by = "geoid")
ggplot() +
geom_sf(data = sqf_data_sf_tract_level, aes(fill = pc_arrest), color = "black", size = 0.3) +
scale_fill_viridis_c(
name = "% Stops Ending in Arrest",
option = "inferno",
na.value = "white"
) +
theme_void() +
labs(title = "Percentage of Stops Ending in Arrest by NYC Census Tract")
# non hispanic black
ggplot(nyc_tract_shp) +
geom_sf(aes(fill = pop_black_est)) +
scale_fill_viridis_c(
name = "Non-Hispanic Black Population",
option = "inferno"
) +
theme_void() +
labs(title = "Non-Hispanic Black Population by Census Tract, ACS 2013-2017")
# hispanic any
ggplot(nyc_tract_shp) +
geom_sf(aes(fill = pop_hisp_est)) +
scale_fill_viridis_c(
name = "Hispanic Any Race Population",
option = "inferno"
) +
theme_void() +
labs(title = "Hispanic Any Race Population by Census Tract, ACS 2013-2017")
# non
ggplot(nyc_tract_shp) +
geom_sf(aes(fill = pop_asian_est)) +
scale_fill_viridis_c(
name = "Non-hispanic Asian Population",
option = "inferno"
) +
theme_void() +
labs(title = "Non-hispanic Asian  Population by Census Tract, ACS 2013-2017")
# pop age 25 years or older with at least bachelors degree
ggplot(nyc_tract_shp) +
geom_sf(aes(fill = pop_ba_above_est)) +
scale_fill_viridis_c(
name = "Population Aged >= 25 with at Least Bachelors Degree",
option = "inferno"
) +
theme_void() +
labs(title = "Population Aged >= 25 with at least a Bachelor's Degree by Census Tract, ACS 2013-2017")
# income below pov
# pop age 25 years or older with at least bachelors degree
ggplot(nyc_tract_shp) +
geom_sf(aes(fill = pop_inpov_est)) +
scale_fill_viridis_c(
name = "Population With Income Below Poverty Line",
option = "inferno"
) +
theme_void() +
labs(title = "Population with Income Below Poverty Line, ACS 2013-2017")
# check current dim
dim(sqf_data)
sqf_data <- sqf_data %>%
# left join selected spatial features from the sf object into sqf_data
left_join(sqf_data_sf %>% select(stop_id, pop_ba_above_est, pop_inpov_est, pop_asian_est, pop_hisp_est, pop_black_est), by = "stop_id") %>%
# drop x,y coords and geometry as we use census tract for spatial info
select(-c("stop_location_x", "stop_location_y", "geometry")) %>%
# drop obs with missing values in these spatial features
filter(!if_any(everything(), is.na))
# check new dim
dim(sqf_data)
# set seed for reproducibility
set.seed(1)
# set y and two predictor matrices (unsplit)
y <- sqf_data$suspect_arrested_flag
X <- model.matrix(~ . - suspect_arrested_flag, data = sqf_data)
X_subset <- X[, !grepl("^(search|physical_force)", colnames(X)) &
!colnames(X) %in% c("frisked_flag", "firearm_flag", "knife_cutter_flag",
"other_weapon_flag", "weapon_found_flag")]
# perform train-test split
train_index <- createDataPartition(y, p = 0.7, list = FALSE)
y_train <- y[train_index]
y_test <- y[-train_index]
X_train <- X[train_index,]
X_test <- X[-train_index, ]
X_train_subset <- X_subset[train_index, ]
X_test_subset <- X_subset[-train_index, ]
# print lengths and dimensions
cat("Length of y_train:", length(y_train), "\n")
cat("Length of y_test:", length(y_test), "\n")
cat("Dimensions of X_train:", dim(X_train), "\n")
cat("Dimensions of X_test:", dim(X_test), "\n")
cat("Dimensions of X_train_subset:", dim(X_train_subset), "\n")
cat("Dimensions of X_test_subset:", dim(X_test_subset), "\n")
# check balance of y
cat("Balance of y_train:\n")
print(table(y_train))
cat("Balance of y_test:\n")
print(table(y_test))
#logistic regression
#X_train <- as.data.frame(X_train)
#logit_all  <- glm(y_train ~ . , data = X_train , family = binomial(logit))
#summary(logit_all)
#logistic regression using subset of variables
#logit_sub  <- glm(y_train ~ -1+ ?? +  ??, data = X_train , family = binomial(logit))
#summary(logit_sub)
#predict_all <- predict(logit_all, newdata = X_test, type = "response")
#predict_sub <- predict(logit_sub, newdata = X_test, type = "response")
# run lasso on training data to collect coefficients
lasso <- cv.glmnet(x=X_train, y=y_train, alpha = 1, family="binomial", type.measure = "class")
# optimal lambda
lasso_lambda_min <- lasso$lambda.min
cat("Optimal Lambda for Lasso:", lasso_lambda_min, "\n")
# plot misclassification error against log lambda
plot(lasso)
title(main = "Cross-Validation Misclassification Error",
sub = "Optimal Lambda Highlighted in Red",
xlab = "Log(Lambda)",
ylab = "Misclassification Error")
# plot coefficients
plot(lasso$glmnet.fit, xvar = "lambda", label = TRUE)
title(main = "Coefficient Shrinkage Path for LASSO",
sub = "Optimal Lambda Highlighted in Red",
xlab = "Log(Lambda)",
ylab = "Coefficients")
# get fitted probabilities, using best lambda
lasso_predict <- predict(lasso, s = lasso_lambda_min, X_test, type = "response")
# add variable for predicted classes
lasso_y_hat <- ifelse(lasso_predict > 0.5, 1, 0)
# define function to generate and plot confusion matrix
generate_cm <- function(true, predicted, title) {
cm <- as.data.frame(table(True = true, Predicted = predicted)) %>%
group_by(Predicted) %>%
mutate(Predicted_pct = Freq / sum(Freq))
print(cm)
plot <- ggplot(data = cm, mapping = aes(x = ordered(True, c(1, 0)), y = Predicted, fill = Predicted_pct)) +
geom_tile() +
geom_text(aes(label = round(Predicted_pct, 2)), color = 'white') +
scale_fill_gradient(low = "blue", high = "red", name = "Rel. Freq.") +
xlab("True") +
ylab("Predicted") +
labs(title = title) +
theme_minimal()
# print the plot
print(plot)
}
# plot confusion matrix for lasso regression out of sample
generate_cm(y_test, lasso_y_hat, "Confusion Matrix for LASSO (Full Model)")
# compute ROC
lasso_roc_full <- roc(response = y_test, predictor = lasso_predict)
# plot ROC
# run lasso on training data to collect coefficients
lasso_subset <- cv.glmnet(x=X_train_subset, y=y_train, alpha = 1, family="binomial", type.measure = "class")
lasso_lambda_min_subset <- lasso_subset$lambda.min
# plot misclassification error against log lambda
plot(lasso_subset)
title(main = "Cross-Validation Misclassification Error",
sub = "Optimal Lambda Highlighted in Red",
xlab = "Log(Lambda)",
ylab = "Misclassification Error")
# plot coefficients
plot(lasso_subset$glmnet.fit, xvar = "lambda", label = TRUE)
title(main = "Coefficient Shrinkage Path for LASSO",
sub = "Optimal Lambda Highlighted in Red",
xlab = "Log(Lambda)",
ylab = "Coefficients")
# get fitted probabilities
lasso_predict_subset <- predict(lasso_subset, s = lasso_lambda_min_subset, X_test_subset, type = "response")
# add variable for predicted classes
lasso_y_hat_subset <- ifelse(lasso_predict_subset > 0.5, 1, 0)
# plot confusion matrix for lasso subset regression out of sample
generate_cm(y_test, lasso_y_hat_subset, "Confusion Matrix for LASSO (Subset Model)")
# new function to compute and plot combined roc for full and subset models
plot_combined_roc <- function(full_roc, subset_roc, full_label, subset_label) {
# plot the full model roc curve
plot(y = full_roc$sensitivities, x = 1 - full_roc$specificities, type = 'l',
col = 'blue', lwd = 2, xlab = 'False Positive Rate', ylab = 'True Positive Rate',
main = 'ROC Curves')
# add the subset model roc curve
lines(y = subset_roc$sensitivities, x = 1 - subset_roc$specificities, col = 'green', lwd = 2)
# add random guess line
abline(a = 0, b = 1, lty = 2, col = 'gray')
# add grid
grid()
# add legend
legend('bottomright',
legend = c(sprintf("%s AUC: %.3f", full_label, auc(full_roc)),
sprintf("%s AUC: %.3f", subset_label, auc(subset_roc)),
'Random Guess'),
lty = c(1, 1, 2), lwd = 2, col = c('blue', 'green', 'gray'))
# add bounding box
box()
}
# compute roc object for subset model
lasso_roc_subset <- roc(response = y_test, predictor = lasso_predict_subset)
# plot combined roc curve for lasso models
plot_combined_roc(lasso_roc_full, lasso_roc_subset, "Lasso Full", "Lasso Subset")
# run ridge regression on training data to collect coefficients
ridge <- cv.glmnet(x = X_train, y = y_train, alpha = 0, family = "binomial", type.measure = "class")
ridge_lambda_min <- ridge$lambda.min
cat("optimal lambda for ridge:", ridge_lambda_min, "\n")
# plot misclassification error against log lambda
plot(ridge)
title(main = "cross-validation misclassification error (ridge)",
sub = "optimal lambda highlighted in red",
xlab = "log(lambda)",
ylab = "misclassification error")
# plot coefficients
plot(ridge$glmnet.fit, xvar = "lambda", label = TRUE)
title(main = "coefficient shrinkage path for ridge",
sub = "optimal lambda highlighted in red",
xlab = "log(lambda)",
ylab = "coefficients")
# get fitted probabilities
ridge_predict <- predict(ridge, s = ridge_lambda_min, X_test, type = "response")
nrow(ridge_predict)
# add variable for predicted classes
ridge_y_hat <- ifelse(ridge_predict > 0.5, 1, 0)
# plot confusion matrix for ridge regression out of sample
generate_cm(y_test, ridge_y_hat, "Confusion Matrix for Ridge (Full Model)")
# compute roc object for full ridge model
ridge_roc_full <- roc(response = y_test, predictor = ridge_predict)
