
---
title: 'Final Project: NYC Arrests Prediction'
author: "Mira√ß Arda Balaban, Rachel Kane, Lucas Mordue, Gretchen Moulton"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  html_document:
    toc: true
    number_sections: true
    toc_float: true
    df-print: paged
    fig-width: 9
    fig-height: 6
    page-layout: full
---

# Abstract

The goal of our project is to predict Y. *edit, include key findings*

# Introduction

* **Target variable:**

* **Motivation:** *edit here re relevance, cite some literature*

* **Relevant features:** *edit*

* **Methods:** *edit*

# Data Description

 We use the [New York Police Department Stop, Question and Frisk Data](https://www.nyc.gov/site/nypd/stats/reports-analysis/stopfrisk.page) from 2023.
 
Each observation of this dataset is a unique stop made by an NYPD police officer in 2023 as part of the SQF programme.

* How data is collected & what universe is/is not observed

* Temporal and spatial span.

The data has 16971 observations and 82 features, as shown below.

We begin the project by installing and loading in the necessary libraries.

```{r}
# Install and load required packages
if (!require("pacman")) install.packages("pacman")
library(pacman)

p_load(readxl, dplyr, ggplot2, knitr, lubridate, tidyr, sf, httr, caret, glmnet, stringr, remotes, RColorBrewer, viridis, scales, classInt)
```

We proceed by setting up the file path and importing the data.

```{r}
# get raw content of the file 
response <- GET("https://raw.githubusercontent.com/rrachelkane/data-science-group-project/main/data/sqf-2023.xlsx")

# retrieve the .xlsx file
if (status_code(response) == 200) {
  # create a temporary file to save the downloaded content
  temp_file <- tempfile(fileext = ".xlsx")
  
  # Write the raw content to the temporary file
  writeBin(content(response, "raw"), temp_file)
  
  # Read the Excel file from the temporary file
  sqf_data <- read_xlsx(temp_file)
  
  # View the first few rows of the data
  head(sqf_data)
} else {
  stop("Failed to download the file.")
}

# check original dimensions
dim(sqf_data)

# view head
head(sqf_data)
```

# Data Cleaning

## Column Names

First, we change column names from strictly upper case to strictly lower case, because it's cuter.

```{r}
colnames(sqf_data) <- tolower(colnames(sqf_data))

# check
colnames(sqf_data)[1:3]
```

## Relevant Columns

Next, we drop all columns which cannot be used for our prediction question, as they are realized **after** the outcome of interest, namely Y, or are irrelevant for other reasons. We drop spatial features other than police precinct of the stop and x and y coordinates of the stop.

```{r}
sqf_data <- sqf_data %>% 
  select(- c("stop_frisk_date", "record_status_code", "supervising_action_corresponding_activity_log_entry_reviewed", "stop_location_sector_code", "stop_location_apartment", "stop_location_full_address", "stop_location_patrol_boro_name", "stop_location_street_name", "suspect_other_description", "observed_duration_minutes", "stop_duration_minutes"))

# officer not explained stop description

# this needs to be edited depending on what we choose for y, might drop  firearm etc flags, physical_force_ etc flags, suspects_actions (unclear when it occurs)

# check new dim
dim(sqf_data)
```

## Missing Values 

First, we note that there is only 1 column with any instance of an `NA` value.

```{r}
na_cols <- colMeans(is.na(sqf_data)) * 100 
na_cols[na_cols > 0]
```

The process generating the missingness of `demeanor_of_person_stopped` is unclear. Imputation of this would be difficult, so we drop this column.

```{r}
# drop 
sqf_data <- sqf_data %>% 
  select(-("demeanor_of_person_stopped"))

# check new dim
dim(sqf_data)
```

Additionally, there are many observations in the data with values == `(null)` across different columns.

```{r}
# get % of nulls, in columns with at least one null
null_cols <- (colMeans(sqf_data == "(null)") * 100)[colMeans(sqf_data == "(null)") * 100 > 0]

# make df for plot
null_cols_df <- data.frame(Feature = names(null_cols), Percentage = null_cols)

dim(null_cols_df)

# order for plot
null_cols_df$Feature <- factor(null_cols_df$Feature, 
                              levels = null_cols_df$Feature[order(null_cols_df$Percentage, decreasing = FALSE)])

# plot
ggplot(null_cols_df, aes(x = Feature, y = Percentage)) +
  geom_bar(stat = "identity", fill = "lightblue", color = "darkblue") +
  labs(title = "Percentage of (null) Values per Column", 
       x = "Columns", 
       y = "Percentage of (null) Values") +
  coord_flip() +  # Flip coordinates
  theme_minimal()
```

Note, however, that not all of these `(null)` observations are equivalent:

* in some columns - particularly those with lower percentages of `(null)` values, `(null)` means the data are **genuinely effectively `NA`**, as there are instances of both "Y" and "N" (for binary variable for example), alongside `(null)`.

```{r}
sqf_data %>% 
  group_by(ask_for_consent_flg) %>% 
  summarise(N = n()) %>% 
  kable()
```

* whereas in other cases, the `null` values are actually used as "N".

```{r}
print(unique(sqf_data$firearm_flag))

sqf_data %>% 
  group_by(weapon_found_flag, firearm_flag) %>% 
  summarise(N = n()) %>% 
  kable()
```

Note here that even though a `firearm_flag` has a `"Y"` entry and `weapon_found_flag` has a `"N"` entry, this is not necessarily incorrect, as the officer can have identified a firearm without having to carry out a frisk nor a `search.

We deal with these cases of `(null)` separately:

* we replace the second type of `(null)` with `"N"` values

```{r}
# initialize empty vector
null_2 <- c()

# loop through columns
for (col in names(sqf_data)) {
  # get unique values of the col
  unique_values <- unique(sqf_data[[col]])
  
  # if the unique values are exactly "Y" and "(null)"
  if (all(unique_values %in% c("Y", "(null)")) && length(unique_values) == 2) {
    null_2 <- c(null_2, col)  # add column name to null_2
  }
}

# check n of type 2 nulls
length(null_2)

# pre-clean check
print(unique(sqf_data$firearm_flag))

# replace these nulls with Ns
sqf_data <- sqf_data %>%
  mutate(across(all_of(null_2), ~ ifelse(. == "(null)", "N", .)))

# post-clean check
print(unique(sqf_data$firearm_flag))
```
  
* now replace the first type with actual `NA` values:

```{r}
# initialize empty vector
null_1 <- c()

# loop through columns
for (col in names(sqf_data)) {
  
  # for columns not in null_2
  if (!(col %in% null_2)) {
    # if "(null)" is present in the column
    if ("(null)" %in% sqf_data[[col]]) {
      null_1 <- c(null_1, col)  # add column name to the vector
    }
  }
}

# check length
length(null_1)

# pre-clean check
print(unique(sqf_data$ask_for_consent_flg))

# replace these with NAs
sqf_data <- sqf_data %>%
  mutate(across(all_of(null_1), ~ ifelse(. == "(null)", NA, .)))

# post-clean check
print(unique(sqf_data$ask_for_consent_flg))
```

Now, we percentage of actual missing values, correctly identified by `"NA"`:

```{r}
# get % of NAs, in columns with at least one NA
na_cols <- (colMeans(is.na(sqf_data)) * 100)[colMeans(is.na(sqf_data)) * 100 > 0]

# make df for plot
na_cols_df <- data.frame(Feature = names(na_cols), Percentage = na_cols)

# order for plot
na_cols_df$Feature <- factor(na_cols_df$Feature, 
                              levels = na_cols_df$Feature[order(na_cols_df$Percentage, decreasing = FALSE)])

# plot
ggplot(na_cols_df, aes(x = Feature, y = Percentage)) +
  geom_bar(stat = "identity", fill = "#F8566D", color = "black") +
  labs(title = "Percentage of NA Values per Column", 
       x = "Columns", 
       y = "Percentage of NA Values") +
  coord_flip() +  # Flip coordinates
  theme_minimal()
```

Given the above, we

* drop columns where more than 25% of observations are missing

```{r}
sqf_data <- sqf_data %>% 
  select(-all_of(names(na_cols[na_cols > 25])))

dim(sqf_data)
```

* drop the remaining observations where there are missing values

```{r}
sqf_data <- sqf_data %>%
  filter(!if_any(everything(), is.na))

dim(sqf_data)
```

## Coding of Binary Variables

We change the coding of binary variables from `"Y"` and `"N"`:

```{r}
# pre check
print(unique(sqf_data$frisked_flag))

# clean Ys and Ns and set as numeric
sqf_data <- sqf_data %>%
  mutate(across(
    where(~ all(. %in% c("Y", "N"))), 
    ~ as.numeric(ifelse(. == "Y", 1, ifelse(. == "N", 0, NA)))  #
  ))

# post check
print(unique(sqf_data$frisked_flag))
```

## Other Column Cleaning

We extract the hour of the date from `stop_frisk_time`:

```{r}
sqf_data <- sqf_data %>%
  mutate(stop_hour = as.factor(str_extract(stop_frisk_time, "^\\d{2}")))

print(unique(sqf_data$stop_hour))

# remove stop_frisk_time ?
sqf_data <- sqf_data %>% 
  select(- "stop_frisk_time")
```

We also convert other relevant variables to factors as needed:

```{r}
# convert character columns to factors, except for stop location x and y
sqf_data <- sqf_data %>%
  mutate(across(
    .cols = where(is.character) & !c("stop_location_x", "stop_location_y"),
    .fns = as.factor
  ))

# note delete eye color etc here if not using for prediction (drop to begin with)
```

We also make height and age numeric:

```{r}
sqf_data <- sqf_data %>% 
  mutate(suspect_reported_age = as.numeric(suspect_reported_age),
         suspect_height = as.numeric(suspect_height),
         suspect_weight = as.numeric(suspect_weight))
```

# Exploratory Data Analysis

## Basic Summary Statistics

Here we explore some basic summary statistics for the entire dataset.

```{r}
# check dim again
dim(sqf_data)

# tabulation of y = arrest
sqf_data %>% 
  group_by(suspect_arrested_flag) %>% 
  summarise(N = n(),
            Pc = N / nrow(sqf_data) * 100) %>% 
  arrange(desc(N)) %>% 
  kable(booktabs = TRUE, col.names = c("Suspect Arrested", "N Stops", "% Total Stops"), align = "l")

# sex
ggplot(sqf_data, aes(x = suspect_sex, fill = suspect_sex)) +
  geom_bar() +
  labs(
    title = "Distribution of Suspect Sex", 
    x = "Sex", 
    y = "Count"
  ) +
  theme_minimal() +
  scale_fill_manual(
    values = c("MALE" = "lightblue", "FEMALE" = "pink") 
  ) +
  theme(legend.position = "none")

ggplot(sqf_data, aes(x = suspect_sex, fill = factor(suspect_arrested_flag))) +
  geom_bar(position = "fill") +
  labs(title = "Distribution of Suspect Sex", 
       x = "Sex", 
       y = "Count") +
  theme_minimal() +
  scale_y_continuous(labels = scales::percent) +
  scale_fill_brewer(type = "qual", palette = "Pastel2", name = "Suspect Arrested") 

# race
ggplot(sqf_data, aes(x = suspect_race_description, fill = factor(suspect_race_description))) +
  geom_bar() +
  labs(
    title = "Distribution of Suspect Race", 
    x = "Race",  # Updated x-axis label
    y = "Count"
  ) +
  theme_minimal() +
  scale_fill_brewer(type = "qual", palette = "Pastel1", name = "Suspect Arrested") +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    legend.position = "none"
  )

# age

# height

# weight

# time of stop

# arrests by race
# not stacked
ggplot(data = sqf_data, aes(x = suspect_race_description, fill = factor(suspect_arrested_flag))) +
  geom_bar() +
  coord_flip() +
  theme_minimal() +
  xlab("Suspect Race") +
  ylab("N Observations") +
  scale_fill_brewer(type = "qual", palette = "Pastel1", name = "Suspect Arrested") +
  labs(title = "Suspect Arrested, By Race")

# stacked
ggplot(data = sqf_data, aes(x = suspect_race_description, fill = factor(suspect_arrested_flag))) +
  geom_bar(position = "fill") +
  coord_flip() +
  scale_y_continuous(labels = scales::percent) +
  theme_minimal() +
  xlab("Suspect Race") +
  ylab("N Observations") +
  scale_fill_brewer(type = "qual", palette = "Pastel1", name = "Suspect Arrested") +
  labs(title = "Suspect Arrested, By Race")

# add here suspected crime description

# NB - others X/predictors of interest here - grouped tables, hists, scatters, corr matrices

# check for any outliers and transform as needed

# generate features as needed

```

## Spatial Data Visualisation

We first clean the data for spatial mapping using the `sf` and `nycgeo` packages. 

We use this to obtain information about the stops at the census tract level, due to its granularity and the availability of population statistics at this level. **insert why relevant for prediction**

```{r}
# drop 7 observations which have incorrect spatial info
sqf_data <- sqf_data %>% 
  filter(stop_location_x > 0)

dim(sqf_data)

# make spatial object for mapping
sqf_data_sf <- st_as_sf(sqf_data, 
                        coords = c("stop_location_x", "stop_location_y"), 
                        crs = 2263)  #  crs for New York (EPSG:2263)

# load in nta-level shapefile
remotes::install_github("mfherman/nycgeo")
library(nycgeo)
nyc_tract_shp <- nycgeo::nyc_boundaries(geography = "tract", add_acs_data = TRUE)

# check crs
st_crs(nyc_tract_shp)$epsg
```

### Stop-Level Maps

```{r}
# plot data onto shapefile by arrest status
ggplot() +
  geom_sf(data = nyc_tract_shp, fill = "lightblue", color = "black", size = 0.3) +
  geom_sf(data = sqf_data_sf, aes(color = as.factor(suspect_arrested_flag)), size = 0.7) +
  scale_color_manual(values = c("red", "seagreen"),  
                     labels = c("Arrested", "Not Arrested")) +
  theme_minimal() +
  labs(title = "NYC Police Stops by Arrest Status") +
  theme(plot.title = element_text(hjust = 0.5),
        plot.subtitle = element_text(hjust = 0.5),
        legend.title = element_blank())
```

### Tract Level Maps

Perhaps it is more informative to view the percentage of stops ending in arrest at the tract level:

```{r}
# join datasets to assign each stop to a tract
sqf_data_sf <- st_join(sqf_data_sf, nyc_tract_shp)
dim(sqf_data_sf)

# aggregate to tract level
sqf_data_sf_tract_level <- sqf_data_sf %>%
  filter(!is.na(geoid)) %>%
  group_by(geoid) %>%
  summarize(pc_arrest = (sum(suspect_arrested_flag) / n()) * 100)

# join with shp for mapping
sqf_data_sf_tract_level <- nyc_tract_shp %>%
  st_join(sqf_data_sf_tract_level, by = "geoid")
  
ggplot() +
  geom_sf(data = sqf_data_sf_tract_level, aes(fill = pc_arrest), color = "black", size = 0.3) +
  scale_fill_viridis_c(
    name = "% Stops Ending in Arrest",
    option = "inferno",
    na.value = "white"
  ) +
  theme_void() +
  labs(title = "Percentage of Stops Ending in Arrest by NYC Census Tract")

```

### Tract-Level Population Predictors

Additionally, the `nycgeo` package allows us to link with neighbour tabulation area level American Community Survey (2013-2017) population statistics.

We visualize those here:

```{r}
# non hispanic black
ggplot(nyc_tract_shp) +
  geom_sf(aes(fill = pop_black_est)) +
  scale_fill_viridis_c(
    name = "Non-Hispanic Black Population",
    option = "inferno"
  ) +
  theme_void() +
  labs(title = "Non-Hispanic Black Population by Census Tract, ACS 2013-2017")

# hispanic any
ggplot(nyc_tract_shp) +
  geom_sf(aes(fill = pop_hisp_est)) +
  scale_fill_viridis_c(
    name = "Hispanic Any Race Population",
    option = "inferno"
  ) +
  theme_void() +
  labs(title = "Hispanic Any Race Population by Census Tract, ACS 2013-2017")

# non
ggplot(nyc_tract_shp) +
  geom_sf(aes(fill = pop_asian_est)) +
  scale_fill_viridis_c(
    name = "Non-hispanic Asian Population",
    option = "inferno"
  ) +
  theme_void() +
  labs(title = "Non-hispanic Asian  Population by Census Tract, ACS 2013-2017")

# pop age 25 years or older with at least bachelors degree
ggplot(nyc_tract_shp) +
  geom_sf(aes(fill = pop_ba_above_est)) +
  scale_fill_viridis_c(
    name = "Population Aged >= 25 with at Least Bachelors Degree",
    option = "inferno"
  ) +
  theme_void() +
  labs(title = "Population Aged >= 25 with at least a Bachelor's Degree by Census Tract, ACS 2013-2017")

# income below pov
# pop age 25 years or older with at least bachelors degree
ggplot(nyc_tract_shp) +
  geom_sf(aes(fill = pop_inpov_est)) +
  scale_fill_viridis_c(
    name = "Population With Income Below Poverty Line",
    option = "inferno"
  ) +
  theme_void() +
  labs(title = "Population with Income Below Poverty Line, ACS 2013-2017")
```

We keep only these predictors from the ACS data as predictors for our analysis, as shown below.

```{r}
# check current dim
dim(sqf_data)

sqf_data <- sqf_data %>%
  # left join selected spatial features from the sf object into sqf_data
  left_join(sqf_data_sf %>% select(stop_id, geoid, pop_ba_above_est, pop_inpov_est, pop_asian_est, pop_hisp_est, pop_black_est), by = "stop_id") %>% 
  # drop x,y coords and geometry as we use census tract for spatial info
  select(-c("stop_location_x", "stop_location_y", "geometry")) %>% 
  # drop obs with missing values in these spatial features
  filter(!if_any(everything(), is.na))

# check new dim
dim(sqf_data)
```

# Regression Analysis

## LASSO

We first run the following model: **insert formula**

```{r}
# set seed for reproducibility
set.seed(1)

# split sample - depends on if using 2022 to train?

# insert lucas code
regressors_1 <- c()


```

* state models, assumptions, evaluation criteria blah blah blah
* run + check model diagnostics

## Ridge

## Elastic-Net

## Non-linear Methods

# Conclusion
