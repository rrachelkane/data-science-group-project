
---
title: 'Final Project: NYC Arrests Prediction'
author: "Miraç Arda Balaban, Rachel Kane, Lucas Mordue, Gretchen Moulton"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  html_document:
    toc: true
    number_sections: true
    toc_float: true
    df-print: paged
    fig-width: 9
    fig-height: 6
    page-layout: full
---

# Abstract
In the early 2000s, the New York City Police Department was required to publicly report data on their stop-and-frisk program. This program allows officers to temporarily stop, question, and potentially frisk/search civilians they suspect may have committed a crime without needing evidence or a warrant. The stops may result in release, arrest, or criminal summons. As civilians can be stopped without evidence, we want to explore what variables are the most accurate prediction of the stop resulting in arrest. In order to predict arrests, we will use classification methods. WRITE MORE ABOUT METHODS, FINDINGS, and CONCLUSIONS

The goal of our project is to predict Y. *edit, include key findings*

# Introduction
For over 20 years, civilians in NYC may be stopped, question, and searched by the police based solely on suspicion without any evidence. From 2002 – 2012, under Mayor Bloomberg’s administration, stops were more rampant, reaching a peak in 2011 with 685,724 reported stops (NYCLU 2024). With changes to administrations, the number of stops has decreased in recent years with 15,102 and 16,791 stops in 2022 and 2023 respectively. 
The stop-and-frisk program has been highly criticized as it does not require evidence to stop individuals and may allow police to use racial profiling tactics to target individuals. For example, the American Civil Liberties Union of New York (NYCLU) has criticized the program as their investigation of the data found a disproportionate number of civilians stopped are people of color, in particular Black civilians, and some stops have resulted in police misconduct. As the stops do not require previous evidence or warrants, we are interested in predicting which stops result in arrest, particularly focusing on factors gathered before the stop begins that may predict arrest, such as demographic characteristics of civilians, location, time of day, and attributes of the police officers. 

* **Target variable:** Our goal is to predict `arrested_flag`, which is a binary indicator which equals 1 if the individual who was stopped was arrested, and 0 otherwise.^[In the raw data, the flag's entries are actually `"Y"` and `"N"`, but we clean this.]

* **Motivation:** *edit here re relevance, cite some literature*

* **Relevant features:** *edit*

* **Methods:** *edit*

# Data Description

We use the [New York Police Department Stop, Question and Frisk Data](https://www.nyc.gov/site/nypd/stats/reports-analysis/stopfrisk.page) from 2023.
 
This is a stop-level dataset; each observation (row) corresponds to a unique stop made by an NYPD police officer in 2023 as part of the SQF programme.

* How data is collected & what universe is/is not observed (some note here on the forms they have to fill out to record the stop, refer Precinct and Prejudice paper on why some stops may not actually be recorded)

* **Temporal span**: the entire year of 2023.

* **Spatial span**: the 77 NYPD police precincts covering all 5 boroughs of NYC.  

* **Dimension**: The data has 16971 observations and 82 features, as shown below.

We begin the project by installing and loading in the necessary libraries.

```{r}
# Install and load required packages
if (!require("pacman")) install.packages("pacman")
library(pacman)

p_load(readxl, dplyr, ggplot2, knitr, lubridate, tidyr, sf, httr, caret, glmnet, stringr, remotes, RColorBrewer, viridis, scales, classInt, forcats, pROC, randomForest)
```

We proceed by setting up the file path and importing the data from our project's GitHub repository.

```{r}
# get raw content of the file 
response <- GET("https://raw.githubusercontent.com/rrachelkane/data-science-group-project/main/data/sqf-2023.xlsx")

# retrieve the .xlsx file
if (status_code(response) == 200) {
  # create a temporary file to save the downloaded content
  temp_file <- tempfile(fileext = ".xlsx")
  
  # Write the raw content to the temporary file
  writeBin(content(response, "raw"), temp_file)
  
  # Read the Excel file from the temporary file
  sqf_data <- read_xlsx(temp_file)
  
  # View the first few rows of the data
  head(sqf_data)
} else {
  stop("Failed to download the file.")
}

# check original dimensions
dim(sqf_data)

# view head
head(sqf_data)
```

# Data Cleaning

## Column Names

First, we change column names from strictly upper case to strictly lower case, because it's cuter.

```{r}
colnames(sqf_data) <- tolower(colnames(sqf_data))

# check
colnames(sqf_data)[1:3]
```

## Relevant Columns

Next, we drop all columns which cannot be used for our prediction question, as they are realized **after** the outcome of interest, namely `arrested_flag`, or are irrelevant for other reasons. We drop spatial features other than x and y coordinates of the stop, as this is the most granular spatial information and we use this to map each stop to census tracts to obtain demographic information corresponding to the location of the stop.These dropped spatial features also have high cardinality, which would add many dummies to our model, undermining computational efficiency.

```{r}
sqf_data <- sqf_data %>% 
  select(- c("stop_frisk_date", "record_status_code", "supervising_action_corresponding_activity_log_entry_reviewed", "stop_location_sector_code", "stop_location_apartment", "stop_location_full_address", "stop_location_patrol_boro_name", "stop_location_street_name", "suspect_other_description", "observed_duration_minutes", "stop_duration_minutes", "summons_issued_flag", "supervising_officer_command_code", "issuing_officer_command_code", "stop_location_precinct"))

# check new dim
dim(sqf_data)
```

## Missing Values 

First, we note that there is only 1 column with any instance of an `NA` value.

```{r}
na_cols <- colMeans(is.na(sqf_data)) * 100 
na_cols[na_cols > 0]
```

The process generating the missingness of `demeanor_of_person_stopped` is unclear. Imputation of this would be difficult, so we drop this column.

```{r}
# drop 
sqf_data <- sqf_data %>% 
  select(-("demeanor_of_person_stopped"))

# check new dim
dim(sqf_data)
```

Additionally, there are many observations in the data with values == `(null)` across different columns.

```{r}
# get % of nulls, in columns with at least one null
null_cols <- (colMeans(sqf_data == "(null)") * 100)[colMeans(sqf_data == "(null)") * 100 > 0]

# make df for plot
null_cols_df <- data.frame(Feature = names(null_cols), Percentage = null_cols)

dim(null_cols_df)

# order for plot
null_cols_df$Feature <- factor(null_cols_df$Feature, 
                              levels = null_cols_df$Feature[order(null_cols_df$Percentage, decreasing = FALSE)])

# plot
ggplot(null_cols_df, aes(x = Feature, y = Percentage)) +
  geom_bar(stat = "identity", fill = "lightblue", color = "darkblue") +
  labs(title = "Percentage of (null) Values per Column", 
       x = "Columns", 
       y = "Percentage of (null) Values") +
  coord_flip() +  # Flip coordinates
  theme_minimal()
```

Note, however, that not all of these `(null)` observations are equivalent:

* in some columns - particularly those with lower percentages of `(null)` values, `(null)` means the data are **genuinely effectively `NA`**, as there are instances of both "Y" and "N" (for binary variable for example), alongside `(null)`.

```{r}
sqf_data %>% 
  group_by(ask_for_consent_flg) %>% 
  summarise(N = n()) %>% 
  kable()
```

* whereas in other cases, the `null` values are actually used as "N".

```{r}
print(unique(sqf_data$firearm_flag))

sqf_data %>% 
  group_by(weapon_found_flag, firearm_flag) %>% 
  summarise(N = n()) %>% 
  kable()
```

Note here that even though a `firearm_flag` has a `"Y"` entry and `weapon_found_flag` has a `"N"` entry, this is not necessarily incorrect, as the officer can have identified a firearm without having to carry out a frisk nor a `search.

We deal with these cases of `(null)` separately:

* we replace the second type of `(null)` with `"N"` values

```{r}
# initialize empty vector
null_2 <- c()

# loop through columns
for (col in names(sqf_data)) {
  # get unique values of the col
  unique_values <- unique(sqf_data[[col]])
  
  # if the unique values are exactly "Y" and "(null)"
  if (all(unique_values %in% c("Y", "(null)")) && length(unique_values) == 2) {
    null_2 <- c(null_2, col)  # add column name to null_2
  }
}

# check n of type 2 nulls
length(null_2)

# pre-clean check
print(unique(sqf_data$firearm_flag))

# replace these nulls with Ns
sqf_data <- sqf_data %>%
  mutate(across(all_of(null_2), ~ ifelse(. == "(null)", "N", .)))

# post-clean check
print(unique(sqf_data$firearm_flag))
```
  
* now replace the first type with actual `NA` values:

```{r}
# initialize empty vector
null_1 <- c()

# loop through columns
for (col in names(sqf_data)) {
  
  # for columns not in null_2
  if (!(col %in% null_2)) {
    # if "(null)" is present in the column
    if ("(null)" %in% sqf_data[[col]]) {
      null_1 <- c(null_1, col)  # add column name to the vector
    }
  }
}

# check length
length(null_1)

# pre-clean check
print(unique(sqf_data$ask_for_consent_flg))

# replace these with NAs
sqf_data <- sqf_data %>%
  mutate(across(all_of(null_1), ~ ifelse(. == "(null)", NA, .)))

# post-clean check
print(unique(sqf_data$ask_for_consent_flg))
```

Now, we percentage of actual missing values, correctly identified by `"NA"`:

```{r}
# get % of NAs, in columns with at least one NA
na_cols <- (colMeans(is.na(sqf_data)) * 100)[colMeans(is.na(sqf_data)) * 100 > 0]

# make df for plot
na_cols_df <- data.frame(Feature = names(na_cols), Percentage = na_cols)

# order for plot
na_cols_df$Feature <- factor(na_cols_df$Feature, 
                              levels = na_cols_df$Feature[order(na_cols_df$Percentage, decreasing = FALSE)])

# plot
ggplot(na_cols_df, aes(x = Feature, y = Percentage)) +
  geom_bar(stat = "identity", fill = "#F8566D", color = "black") +
  labs(title = "Percentage of NA Values per Column", 
       x = "Columns", 
       y = "Percentage of NA Values") +
  coord_flip() +  # Flip coordinates
  theme_minimal()
```

Given the above, we

* drop columns where more than 25% of observations are missing

```{r}
sqf_data <- sqf_data %>% 
  select(-all_of(names(na_cols[na_cols > 25])))

dim(sqf_data)
```

* drop the remaining observations where there are missing values

```{r}
sqf_data <- sqf_data %>%
  filter(!if_any(everything(), is.na))

dim(sqf_data)
```

## Coding of Binary Variables

We change the coding of binary variables from `"Y"` and `"N"`:

```{r}
# pre check
print(unique(sqf_data$frisked_flag))

# clean Ys and Ns and set as numeric
sqf_data <- sqf_data %>%
  mutate(across(
    where(~ all(. %in% c("Y", "N"))), 
    ~ as.numeric(ifelse(. == "Y", 1, ifelse(. == "N", 0, NA)))  #
  ))

# post check
print(unique(sqf_data$frisked_flag))
```

## Other Column Cleaning

We also bin the time of the stop from `stop_frisk_time`:

```{r}
sqf_data <- sqf_data %>%
  mutate(
    time_of_day = case_when(
      str_extract(stop_frisk_time, "^\\d{2}") %in% c("06", "07", "08", "09", "10", "11") ~ "Morning",
      str_extract(stop_frisk_time, "^\\d{2}") %in% c("12", "13", "14", "15", "16", "17") ~ "Afternoon",
      str_extract(stop_frisk_time, "^\\d{2}") %in% c("18", "19", "20", "21", "22", "23") ~ "Evening",
      str_extract(stop_frisk_time, "^\\d{2}") %in% c("00", "01", "02", "03", "04", "05") ~ "Night",
      TRUE ~ NA_character_  # Handle unexpected or missing values
    ),
    time_of_day = factor(time_of_day, levels = c("Morning", "Afternoon", "Evening", "Night"))  # Ensure factor order
  )

# Verify the result
print(table(sqf_data$time_of_day))

# drop stop frisk time as we will just use time_of_day
sqf_data <- sqf_data %>% 
  select(-"stop_frisk_time")

```

**Note** - do we want to keep stop hour as well? or just use time of day?
-Lucas Answer: I would just use time of day but I would extend shift night along one hour and shorten morning/lengthen evening (or make them uniform blocks of 6 hours)


We also convert other relevant variables to factors as needed:

```{r}
# convert character columns to factors, except for stop location x and y
sqf_data <- sqf_data %>%
  mutate(across(
    .cols = where(is.character) & !c("stop_location_x", "stop_location_y"),
    .fns = as.factor
  ))
```

Next, we will make `suspect_height`, `suspect_weight`, and `suspect_reported_age` numeric keeping in mind the factor levels to ensure R converts them correctly. Then, we will address outliers in the data. 

```{r}
library(dplyr)

# Function to safely convert factor to numeric, handling non-numeric entries
convert_to_numeric <- function(x) {
  # Convert to character to avoid factor levels issues
  x <- as.character(x)
  
  # Replace non-numeric values with NA (e.g., "unknown", "760", "7.6")
  x <- gsub("[^0-9.]", "", x)
  
  # Convert to numeric
  as.numeric(x)
}

# Apply the function to the relevant columns
sqf_data <- sqf_data %>%
  mutate(
    suspect_reported_age = convert_to_numeric(suspect_reported_age),
    suspect_height = convert_to_numeric(suspect_height),
    suspect_weight = convert_to_numeric(suspect_weight)
  )
#Check to make sure successful
summary(sqf_data$suspect_reported_age)
summary(sqf_data$suspect_height)
summary(sqf_data$suspect_weight)
```

Let's plot the density of age to see the distribution. Since we have some outliers, we are going to drop any observations where age is below 10 and above 85. 

```{r}
# Compute density for reported_age
reported_age_density <- density(sqf_data$suspect_reported_age, na.rm = TRUE)

# Plot the density
plot(reported_age_density, 
     main = "Density of Reported Age with Outliers Highlighted", 
     xlab = "Reported Age", 
     ylab = "Density", 
     col = "black", 
     lwd = 2)
grid()

# Identify and highlight outliers (ages < 10 and > 85)
outliers <- sqf_data$suspect_reported_age[sqf_data$suspect_reported_age < 10 | sqf_data$suspect_reported_age > 85]

# Add vertical lines to mark the outlier boundaries
abline(v = c(10, 85), col = "deeppink", lty = 2, lwd = 2)

# Highlight the outlier data points on the plot
points(outliers, rep(0, length(outliers)), col = "deeppink", pch = 19)

#We will drop outlier observations where age is above 85 and below 10.
sqf_data <- sqf_data[sqf_data$suspect_reported_age >= 10 & sqf_data$suspect_reported_age <= 85, ]

```

We plot the density of height to see the distribution and identify outliers. Height appears to be measured in feet and we are going to drop observations below 4 ft and above 7 ft. 

```{r}
# Compute density for suspect_height
height_density <- density(sqf_data$suspect_height, na.rm = TRUE)

# Plot the density
plot(height_density, 
     main = "Density of Height with Outliers Highlighted", 
     xlab = "Height", 
     ylab = "Density", 
     col = "black", 
     lwd = 2)
grid()

# Identify and highlight outliers (e.g., heights below 4 feet and above 7 feet)
outliers <- sqf_data$suspect_height[sqf_data$suspect_height < 4 | sqf_data$suspect_height > 7]

# Add vertical lines to mark the outlier boundaries
abline(v = c(4, 7), col = "darkorchid2", lty = 2, lwd = 2)

# Highlight the outlier data points on the plot
points(outliers, rep(0, length(outliers)), col = "darkorchid2", pch = 19)

#We will drop outlier observations where height is above 7 ft and below 4 ft.
sqf_data <- sqf_data[sqf_data$suspect_height >= 4 & sqf_data$suspect_height <= 7, ]
```

We plot the density of weight to identify outliers. The weight variable appears to be measured in pounds and it appears that some observations could have data entry errors (they are nonsensical). We will drop outliers above 300 lbs and below 90 lbs. 

```{r}
# Compute density for suspect_weight
weight_density <- density(sqf_data$suspect_weight, na.rm = TRUE)

# Plot the density
plot(weight_density, 
     main = "Density of Suspect Weight with Outliers Highlighted", 
     xlab = "Weight", 
     ylab = "Density", 
     col = "black", 
     lwd = 2)
grid()

# Identify and highlight outliers (e.g., weights below 90 lbs and above 300 lbs)
outliers <- sqf_data$suspect_weight[sqf_data$suspect_weight < 90 | sqf_data$suspect_weight > 300]

# Add vertical lines to mark the outlier boundaries
abline(v = c(90, 300), col = "darkturquoise", lty = 2, lwd = 2)

# Highlight the outlier data points on the plot
points(outliers, rep(0, length(outliers)), col = "darkturquoise", pch = 19)

#We will drop outlier observations where height is above 300 lbs. and below 90 lbs.
sqf_data <- sqf_data[sqf_data$suspect_weight >= 90 & sqf_data$suspect_weight <= 300, ]

```

Finally, we will standardize our numerical variables as they use different scales. 

```{r}
# Standardize the numeric variables
sqf_data <- sqf_data %>%
  mutate(
    suspect_reported_age = scale(suspect_reported_age),
    suspect_height = scale(suspect_height),
    suspect_weight = scale(suspect_weight)
  )

# Check if the standardization worked by summarizing the variables
summary(sqf_data$suspect_reported_age)
summary(sqf_data$suspect_height)
summary(sqf_data$suspect_weight)

```

# Exploratory Data Analysis

## Basic Summary Statistics

```{r}
# check dim again
dim(sqf_data)

# tabulation of the dependent variable 
sqf_data %>% 
  group_by(suspect_arrested_flag) %>% 
  summarise(N = n(),
            Pc = N / nrow(sqf_data) * 100) %>% 
  arrange(desc(N)) %>% 
  kable(booktabs = TRUE, col.names = c("Suspect Arrested", "N Stops", "% Total Stops"), align = "l")

# looking at the distribution of sex
ggplot(sqf_data, aes(x = suspect_sex, fill = suspect_sex)) +
  geom_bar() +
  labs(
    title = "Distribution of Suspect Sex", 
    x = "Sex", 
    y = "Count"
  ) +
  theme_minimal() +
  scale_fill_manual(
    values = c("MALE" = "lightblue", "FEMALE" = "pink") 
  ) +
  theme(legend.position = "none")

# sex by arrest status
ggplot(sqf_data, aes(x = suspect_sex, fill = factor(suspect_arrested_flag))) +
  geom_bar(position = "fill") +
  labs(title = "Distribution of Suspect Sex", 
       x = "Sex", 
       y = "Count") +
  theme_minimal() +
  scale_y_continuous(labels = scales::percent) +
  scale_fill_brewer(type = "qual", palette = "Pastel2", name = "Suspect Arrested") 

# empirical cdf of age by sex and arrest status
ggplot(sqf_data, aes(x = suspect_reported_age, color = factor(suspect_arrested_flag))) +
  stat_ecdf(geom = "step") +
  facet_wrap(~ suspect_sex, ncol = 2) +
  scale_color_manual(values = c("0" = "red", "1" = "darkgreen"),
                     labels = c("Not Arrested", "Arrested"),
                     name = "Arrest Outcome") +
  labs(x = "Suspect Reported Age", y = "ECDF", title = "Empirical CDF of Suspect Reported Age, By Sex and Arrest Status") +
  theme_minimal()

# distribution of race
ggplot(sqf_data, aes(x = suspect_race_description, fill = factor(suspect_race_description))) +
  geom_bar() +
  labs(
    title = "Distribution of Suspect Race", 
    x = "Race", 
    y = "Count"
  ) +
  theme_minimal() +
  scale_fill_brewer(type = "qual", palette = "Pastel1", name = "Suspect Arrested") +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    legend.position = "none"
  )

# arrests by race, unstacked
ggplot(data = sqf_data, aes(x = fct_rev(fct_infreq(suspect_race_description)), fill = factor(suspect_arrested_flag))) +
  geom_bar() +
  coord_flip() +
  theme_minimal() +
  xlab("Suspect Race") +
  ylab("N Observations") +
  scale_fill_brewer(type = "qual", palette = "Pastel1", name = "Suspect Arrested") +
  labs(title = "Suspect Arrested, By Race")

# arrests by race, stacked
ggplot(data = sqf_data, aes(x = fct_rev(fct_infreq(suspect_race_description)), fill = factor(suspect_arrested_flag))) +
  geom_bar(position = "fill") +
  coord_flip() +
  scale_y_continuous(labels = scales::percent) +
  theme_minimal() +
  xlab("Suspect Race") +
  ylab("% Observations") +
  scale_fill_brewer(type = "qual", palette = "Pastel1", name = "Suspect Arrested") +
  labs(title = "Suspect Arrested, By Race")

# arrests by suspected crime description, unstacked
ggplot(data = sqf_data, aes(x = fct_rev(fct_infreq(suspected_crime_description)), fill = factor(suspect_arrested_flag))) +
  geom_bar() +
  coord_flip() +
  theme_minimal() +
  xlab("Suspected Crime") +
  ylab("N Observations") +
  scale_fill_brewer(type = "qual", palette = "Pastel1", name = "Suspect Arrested") +
  labs(title = "Suspect Arrested, By Suspected Crime")

# arrests by suspected crime description, stacked
ggplot(data = sqf_data, aes(x = suspected_crime_description, fill = factor(suspect_arrested_flag))) +
  geom_bar(position = "fill") +
  coord_flip() +
  scale_y_continuous(labels = scales::percent) +
  theme_minimal() +
  xlab("Suspected Crime") +
  ylab("% Observations") +
  scale_fill_brewer(type = "qual", palette = "Pastel1", name = "Suspect Arrested") +
  labs(title = "Suspect Arrested, By Suspected Crime")


# to be done:
# time of stop

# cop side stuff

# any other variables of importance

# cormat/heatmaps/PCA

# look for outliers here!

# generate features as needed
```

Here we explore some further summary statistics.

First we will look at our three key numeric variables: `suspect_reported_age`, `suspect_weight`, `suspect_height`.

```{r}
# list of variables to summarize
variables_to_summarize <- c("suspect_reported_age", "suspect_weight", "suspect_height")

# initialize an empty data frame to store results
summary_table <- data.frame()

# loop
for (var in variables_to_summarize) {
  # Create the summary for each variable
  temp_summary <- sqf_data %>%
    summarise(
      Variable = var,  # Add the variable name in the first column
      Min = min(.data[[var]], na.rm = TRUE),
      `1st Qu.` = quantile(.data[[var]], 0.25, na.rm = TRUE),
      Median = median(.data[[var]], na.rm = TRUE),
      Mean = mean(.data[[var]], na.rm = TRUE),
      `3rd Qu.` = quantile(.data[[var]], 0.75, na.rm = TRUE),
      Max = max(.data[[var]], na.rm = TRUE)
    )
  
  # bind the result to the summary table
  summary_table <- bind_rows(summary_table, temp_summary)
}

# print
print(summary_table)
```

We will first look at standardizing our numerical variables as they use different scales. 
*Note - Lucas* Do we have an idea of what these values are - surely there are some issues here if they aren't standardised since they all have a minimum of 1.

```{r}
# list the variables to normalize
variables_to_normalize <- c("suspect_reported_age", "suspect_weight", "suspect_height")

# normalize only these variables
sqf_data <- sqf_data %>%
  mutate(across(all_of(variables_to_normalize), ~ (. - mean(., na.rm = TRUE)) / sd(., na.rm = TRUE), 
                .names = "norm_{col}"))


# list of variables to summarize
variables_to_summarize <- c("norm_suspect_reported_age", "norm_suspect_weight", "norm_suspect_height")

# initialize an empty data frame to store results
summary_table <- data.frame()

# loop through each variable and generate summary statistics
for (var in variables_to_summarize) {
  # create the summary for each variable
  temp_summary <- sqf_data %>%
    summarise(
      Variable = var,  # Add the variable name in the first column
      Min = min(.data[[var]], na.rm = TRUE),
      `1st Qu.` = quantile(.data[[var]], 0.25, na.rm = TRUE),
      Median = median(.data[[var]], na.rm = TRUE),
      Mean = mean(.data[[var]], na.rm = TRUE),
      `3rd Qu.` = quantile(.data[[var]], 0.75, na.rm = TRUE),
      Max = max(.data[[var]], na.rm = TRUE)
    )
  
  # bind the result to the summary table
  summary_table <- bind_rows(summary_table, temp_summary)
}

# print the final summary table
print(summary_table)

# drop old variables
sqf_data <- sqf_data %>% 
 select(-c("suspect_reported_age", "suspect_weight", "suspect_height"))
```

## Spatial Data Visualisation

We first clean the data for spatial mapping using the `sf` and `nycgeo` packages. 

We use this to obtain information about the stops at the census tract level, due to its granularity and the availability of population statistics at this level. **insert why relevant for prediction**

```{r}
# drop 7 observations which have incorrect spatial info
sqf_data <- sqf_data %>% 
  filter(stop_location_x > 0)

dim(sqf_data)

# make spatial object for mapping
sqf_data_sf <- st_as_sf(sqf_data, 
                        coords = c("stop_location_x", "stop_location_y"), 
                        crs = 2263)  #  crs for New York (EPSG:2263)

# load in nta-level shapefile
remotes::install_github("mfherman/nycgeo")
library(nycgeo)
nyc_tract_shp <- nycgeo::nyc_boundaries(geography = "tract", add_acs_data = TRUE)

# check crs
st_crs(nyc_tract_shp)$epsg
```

### Stop-Level Maps

```{r}
# plot data onto shapefile by arrest status
ggplot() +
  geom_sf(data = nyc_tract_shp, fill = "lightblue", color = "black", size = 0.3) +
  geom_sf(data = sqf_data_sf, aes(color = as.factor(suspect_arrested_flag)), size = 0.7) +
  scale_color_manual(values = c("red", "seagreen"),  
                     labels = c("Arrested", "Not Arrested")) +
  theme_minimal() +
  labs(title = "NYC Police Stops by Arrest Status") +
  theme(plot.title = element_text(hjust = 0.5),
        plot.subtitle = element_text(hjust = 0.5),
        legend.title = element_blank())
```

### Tract Level Maps

Perhaps it is more informative to view the percentage of stops ending in arrest at the tract level:

```{r}
# join datasets to assign each stop to a tract
sqf_data_sf <- st_join(sqf_data_sf, nyc_tract_shp)
dim(sqf_data_sf)

# aggregate to tract level
sqf_data_sf_tract_level <- sqf_data_sf %>%
  filter(!is.na(geoid)) %>%
  group_by(geoid) %>%
  summarize(pc_arrest = (sum(suspect_arrested_flag) / n()) * 100)

# join with shp for mapping
sqf_data_sf_tract_level <- nyc_tract_shp %>%
  st_join(sqf_data_sf_tract_level, by = "geoid")
  
ggplot() +
  geom_sf(data = sqf_data_sf_tract_level, aes(fill = pc_arrest), color = "black", size = 0.3) +
  scale_fill_viridis_c(
    name = "% Stops Ending in Arrest",
    option = "inferno",
    na.value = "white"
  ) +
  theme_void() +
  labs(title = "Percentage of Stops Ending in Arrest by NYC Census Tract")

```

### Tract-Level Population Predictors

Additionally, the `nycgeo` package allows us to link with neighbour tabulation area level American Community Survey (2013-2017) population statistics.

**insert justification for why these - demographic stats of location of the stop - might be predictors of arrest **

We visualize those here:

```{r}
# non hispanic black
ggplot(nyc_tract_shp) +
  geom_sf(aes(fill = pop_black_est)) +
  scale_fill_viridis_c(
    name = "Non-Hispanic Black Population",
    option = "inferno"
  ) +
  theme_void() +
  labs(title = "Non-Hispanic Black Population by Census Tract, ACS 2013-2017")

# hispanic any
ggplot(nyc_tract_shp) +
  geom_sf(aes(fill = pop_hisp_est)) +
  scale_fill_viridis_c(
    name = "Hispanic Any Race Population",
    option = "inferno"
  ) +
  theme_void() +
  labs(title = "Hispanic Any Race Population by Census Tract, ACS 2013-2017")

# non
ggplot(nyc_tract_shp) +
  geom_sf(aes(fill = pop_asian_est)) +
  scale_fill_viridis_c(
    name = "Non-hispanic Asian Population",
    option = "inferno"
  ) +
  theme_void() +
  labs(title = "Non-hispanic Asian  Population by Census Tract, ACS 2013-2017")

# pop age 25 years or older with at least bachelors degree
ggplot(nyc_tract_shp) +
  geom_sf(aes(fill = pop_ba_above_est)) +
  scale_fill_viridis_c(
    name = "Population Aged >= 25 with at Least Bachelors Degree",
    option = "inferno"
  ) +
  theme_void() +
  labs(title = "Population Aged >= 25 with at least a Bachelor's Degree by Census Tract, ACS 2013-2017")

# income below pov
# pop age 25 years or older with at least bachelors degree
ggplot(nyc_tract_shp) +
  geom_sf(aes(fill = pop_inpov_est)) +
  scale_fill_viridis_c(
    name = "Population With Income Below Poverty Line",
    option = "inferno"
  ) +
  theme_void() +
  labs(title = "Population with Income Below Poverty Line, ACS 2013-2017")
```

We keep only these predictors from the ACS data as predictors for our analysis, as shown below.

```{r}
# check current dim
dim(sqf_data)

sqf_data <- sqf_data %>%
  # left join selected spatial features from the sf object into sqf_data
  left_join(sqf_data_sf %>% select(stop_id, pop_ba_above_est, pop_inpov_est, pop_asian_est, pop_hisp_est, pop_black_est), by = "stop_id") %>% 
  # drop x,y coords and geometry as we use census tract for spatial info
  select(-c("stop_location_x", "stop_location_y", "geometry")) %>% 
  # drop obs with missing values in these spatial features
  filter(!if_any(everything(), is.na))

# check new dim
dim(sqf_data)
```

# Regression Analysis

We first run LASSO, Ridge and Elastic Net regressions.

We split our sample as [edit].

We implement cross-validation in our training sample as [edit] . We use the default $k=10$ folds for computational efficiency.

As we are operating in a classification setting, we choose the value of the parameter lambda that minimizes the **misclassification error**, and implement this by selecting `type.measure = "class"` in our cv.glmnet regressions.

The cost ratio is: $$C = \frac{L(1,0)}{L(0,1)}$$. 

This measures how costly false negatives are relative to false positives. The optimal decision is $$\hat{y_i} = 1 \iff p_i > {1 \over 1+C}$$

We choose $C=1$ and weight both types of misclassification equally, and so classify according to the most likely class i.e $$p_i > 0.5 \longrightarrow \hat{y} = 1$$.

For each model, we also estimate using a matrix of predictors which excludes predictors related to search, physical force and frisk as [edit].

We evaluate all of our models based on their predictive performance:

* in confusion matrices (in test data)
* in ROC curves and associated AUCs (in test data)

We also look at the variable importance between models.

## LASSO 

**insert formula**

```{r}
# set seed for reproducibility
set.seed(1)

# set y and two predictor matrices (unsplit)
y <- sqf_data$suspect_arrested_flag
X <- model.matrix(~ . - suspect_arrested_flag - stop_id, data = sqf_data) #remove unique id

# set x subset, removing anything that is/might be realized during the stop
X_subset <- X[, !grepl("^(search|physical_force|.*eye_color)", colnames(X)) &
                 !colnames(X) %in% c("frisked_flag", "firearm_flag", "knife_cutter_flag", 
                                     "other_weapon_flag", "weapon_found_flag", "other_contraband_flag")]

colnames(X_subset)

# perform train-test split
train_index <- createDataPartition(y, p = 0.7, list = FALSE)
y_train <- y[train_index]
y_test <- y[-train_index]
X_train <- X[train_index,]
X_test <- X[-train_index, ]
X_train_subset <- X_subset[train_index, ]
X_test_subset <- X_subset[-train_index, ]

# print lengths and dimensions
cat("Length of y_train:", length(y_train), "\n")
cat("Length of y_test:", length(y_test), "\n")
cat("Dimensions of X_train:", dim(X_train), "\n")
cat("Dimensions of X_test:", dim(X_test), "\n")
cat("Dimensions of X_train_subset:", dim(X_train_subset), "\n")
cat("Dimensions of X_test_subset:", dim(X_test_subset), "\n")

# check balance of y
cat("Balance of y_train:\n")
print(table(y_train))
cat("Balance of y_test:\n")
print(table(y_test))

# run lasso on training data to collect coefficients
lasso <- cv.glmnet(x=X_train, y=y_train, alpha = 1, family="binomial", type.measure = "class")

# optimal lambda
lasso_lambda_min <- lasso$lambda.min
cat("Optimal Lambda for Lasso:", lasso_lambda_min, "\n")

# plot misclassification error against log lambda
plot(lasso)
title(main = "Cross-Validation Misclassification Error",
      sub = "Optimal Lambda Highlighted in Red",
      xlab = "Log(Lambda)",
      ylab = "Misclassification Error")

# plot coefficients
plot(lasso$glmnet.fit, xvar = "lambda", label = TRUE)
title(main = "Coefficient Shrinkage Path for LASSO",
      sub = "Optimal Lambda Highlighted in Red",
      xlab = "Log(Lambda)",
      ylab = "Coefficients")

# get fitted probabilities, using best lambda
lasso_predict <- predict(lasso, s = lasso_lambda_min, X_test, type = "response")

# add variable for predicted classes
lasso_y_hat <- ifelse(lasso_predict > 0.5, 1, 0)

# define function to generate and plot confusion matrix
generate_cm <- function(true, predicted, title) {
  cm <- as.data.frame(table(True = true, Predicted = predicted)) %>%
    group_by(Predicted) %>%
    mutate(Predicted_pct = Freq / sum(Freq))
  
  print(cm)
  
  plot <- ggplot(data = cm, mapping = aes(x = ordered(True, c(1, 0)), y = Predicted, fill = Predicted_pct)) +
    geom_tile() +
    geom_text(aes(label = round(Predicted_pct, 2)), color = 'white') +
    scale_fill_gradient(low = "blue", high = "red", name = "Rel. Freq.") +
    xlab("True") +
    ylab("Predicted") +
    labs(title = title) +
    theme_minimal()
  
  # print the plot
  print(plot)
}

# plot confusion matrix for lasso regression out of sample
generate_cm(y_test, lasso_y_hat, "Confusion Matrix for LASSO (Full Model)")

# compute ROC
lasso_roc_full <- roc(response = y_test, predictor = lasso_predict)

# plot ROC
```
*Note - Lucas* Some issues with the graphics
What is the benefit of doing the subset?

### Sensitivity Analysis - Subset of Predictors

```{r}
# run lasso on training data to collect coefficients
lasso_subset <- cv.glmnet(x=X_train_subset, y=y_train, alpha = 1, family="binomial", type.measure = "class")

lasso_lambda_min_subset <- lasso_subset$lambda.min

# plot misclassification error against log lambda
plot(lasso_subset)
title(main = "Cross-Validation Misclassification Error",
      sub = "Optimal Lambda Highlighted in Red",
      xlab = "Log(Lambda)",
      ylab = "Misclassification Error")

# plot coefficients
plot(lasso_subset$glmnet.fit, xvar = "lambda", label = TRUE)
title(main = "Coefficient Shrinkage Path for LASSO",
      sub = "Optimal Lambda Highlighted in Red",
      xlab = "Log(Lambda)",
      ylab = "Coefficients")

# get fitted probabilities
lasso_predict_subset <- predict(lasso_subset, s = lasso_lambda_min_subset, X_test_subset, type = "response")

# add variable for predicted classes
lasso_y_hat_subset <- ifelse(lasso_predict_subset > 0.5, 1, 0)

# plot confusion matrix for lasso subset regression out of sample
generate_cm(y_test, lasso_y_hat_subset, "Confusion Matrix for LASSO (Subset Model)")

# new function to compute and plot combined roc for full and subset models
plot_combined_roc <- function(full_roc, subset_roc, full_label, subset_label) {
  # plot the full model roc curve
  plot(y = full_roc$sensitivities, x = 1 - full_roc$specificities, type = 'l',
       col = 'blue', lwd = 2, xlab = 'False Positive Rate', ylab = 'True Positive Rate',
       main = 'ROC Curves')
  
  # add the subset model roc curve
  lines(y = subset_roc$sensitivities, x = 1 - subset_roc$specificities, col = 'green', lwd = 2)
  
  # add random guess line
  abline(a = 0, b = 1, lty = 2, col = 'gray')
  
  # add grid
  grid()
  
  # add legend
  legend('bottomright',
         legend = c(sprintf("%s AUC: %.3f", full_label, auc(full_roc)),
                    sprintf("%s AUC: %.3f", subset_label, auc(subset_roc)),
                    'Random Guess'),
         lty = c(1, 1, 2), lwd = 2, col = c('blue', 'green', 'gray'))
  
  # add bounding box
  box()
}

# compute roc object for subset model
lasso_roc_subset <- roc(response = y_test, predictor = lasso_predict_subset)

# plot combined roc curve for lasso models
plot_combined_roc(lasso_roc_full, lasso_roc_subset, "Lasso Full", "Lasso Subset")

```

## Ridge 

```{r}
# run ridge regression on training data to collect coefficients
ridge <- cv.glmnet(x = X_train, y = y_train, alpha = 0, family = "binomial", type.measure = "class")

ridge_lambda_min <- ridge$lambda.min
cat("optimal lambda for ridge:", ridge_lambda_min, "\n")

# plot misclassification error against log lambda
plot(ridge)
title(main = "cross-validation misclassification error (ridge)",
      sub = "optimal lambda highlighted in red",
      xlab = "log(lambda)",
      ylab = "misclassification error")

# plot coefficients
plot(ridge$glmnet.fit, xvar = "lambda", label = TRUE)
title(main = "coefficient shrinkage path for ridge",
      sub = "optimal lambda highlighted in red",
      xlab = "log(lambda)",
      ylab = "coefficients")

# get fitted probabilities
ridge_predict <- predict(ridge, s = ridge_lambda_min, X_test, type = "response")
nrow(ridge_predict)

# add variable for predicted classes
ridge_y_hat <- ifelse(ridge_predict > 0.5, 1, 0)

# plot confusion matrix for ridge regression out of sample
generate_cm(y_test, ridge_y_hat, "Confusion Matrix for Ridge (Full Model)")

# compute roc object for full ridge model
ridge_roc_full <- roc(response = y_test, predictor = ridge_predict)
```

### Sensitivity Analysis - Subset

```{r}
# run ridge regression on subset predictors
ridge_subset <- cv.glmnet(x = X_train_subset, y = y_train, alpha = 0, family = "binomial", type.measure = "class")

ridge_lambda_min_subset <- ridge_subset$lambda.min
cat("Optimal lambda for ridge (subset):", ridge_lambda_min_subset, "\n")

# plot misclassification error against log lambda
plot(ridge_subset)
title(main = "Cross-Validation Misclassification Error (Ridge - Subset)",
      sub = "Optimal Lambda Highlighted in Red",
      xlab = "Log(Lambda)",
      ylab = "Misclassification Error")

# plot coefficients
plot(ridge_subset$glmnet.fit, xvar = "lambda", label = TRUE)
title(main = "Coefficient Shrinkage Path for Ridge (Subset)",
      sub = "Optimal Lambda Highlighted in Red",
      xlab = "Log(Lambda)",
      ylab = "Coefficients")

# get fitted probabilities
ridge_predict_subset <- predict(ridge_subset, s = ridge_lambda_min_subset, X_test_subset, type = "response")

# add variable for predicted classes
ridge_y_hat_subset <- ifelse(ridge_predict_subset > 0.5, 1, 0)

# plot confusion matrix for ridge subset regression out of sample
generate_cm(y_test, ridge_y_hat_subset, "Confusion Matrix for Ridge (Subset Model)")

# compute roc object for subset ridge model
ridge_roc_subset <- roc(response = y_test, predictor = ridge_predict_subset)

# plot combined roc curve for ridge models
plot_combined_roc(ridge_roc_full, ridge_roc_subset, "Ridge Full", "Ridge Subset")
```

## Elastic Net

**insert formula**

note no optimization of alpha here, yet
*Note - Lucas* This is where we could use the basis of the earlier en regression I did. I.e. initialise an en with alpha of 0 and then increase alpha and replace it in the model only if it improves accuracy - to discuss
note here - add training accuracy measures? to compare

```{r}
# run elastic net regression on training data to collect coefficients
en <- cv.glmnet(x = X_train, y = y_train, alpha = 0.5, family = "binomial", type.measure = "class")

en_lambda_min <- en$lambda.min
cat("optimal lambda for elastic net:", en_lambda_min, "\n")

# plot misclassification error against log lambda
plot(en)
title(main = "cross-validation misclassification error (elastic net)",
      sub = "optimal lambda highlighted in red",
      xlab = "log(lambda)",
      ylab = "misclassification error")

# plot coefficients
plot(en$glmnet.fit, xvar = "lambda", label = TRUE)
title(main = "coefficient shrinkage path for elastic net",
      sub = "optimal lambda highlighted in red",
      xlab = "log(lambda)",
      ylab = "coefficients")

# get fitted probabilities
en_predict <- predict(en, s = en_lambda_min, X_test, type = "response")
nrow(en_predict)

# add variable for predicted classes
en_y_hat <- ifelse(en_predict > 0.5, 1, 0)

# plot confusion matrix for en regression out of sample
generate_cm(y_test, en_y_hat, "Confusion Matrix for Elastic Net (Full Model)")

# compute roc object for subset ridge model
en_roc_full <- roc(response = y_test, predictor = en_predict)
```


### Tuning Alpha

Potential Alternative to optimising over alpha

```{r}
#alpha_values <- seq(0, 1, by = 0.01)  # Range of alpha to test
#en_opt_best_alpha <- NULL
#en_opt_best_auc <- -Inf  # Initialize with the lowest possible AUC
#en_opt_best_model <- NULL
#en_opt_best_lambda <- NULL

# Iterate over alpha values
#for (alpha in alpha_values) {
  # Train Elastic Net model for the given alpha
  #en_opt <- cv.glmnet(X_train, y_train, alpha = alpha, family = "binomial", type.measure = "class")
  
  # Get the best lambda from cross-validation
  #en_opt_lambda_min <- en_opt$lambda.min
  
  # Predict probabilities on the validation set
  #en_opt_y_prob <- predict(en_opt, s = en_opt_lambda_min, newx = X_test, type = "response")
  
  # Compute AUC
  #en_opt_auc_value <- auc(response = y_test, predictor = en_opt_y_prob)
  
  # Update best alpha if AUC improves
  #if (en_opt_auc_value > best_auc) {
    #en_opt_best_auc <- en_opt_auc_value
    #en_opt_best_alpha <- alpha
    #en_opt_best_model <- en_opt_auc_value
    #en_opt_best_lambda <- en_opt_lambda_min
  #}
  
  # Print progress
  #cat(sprintf("Alpha: %.2f, AUC: %.5f\n", alpha, en_opt_auc_value))
#}

# Output best alpha and its AUC
#cat(sprintf("Best Alpha: %.2f\n", en_opt_best_alpha))
#cat(sprintf("Highest AUC: %.5f\n", en_opt_best_auc))

# Best model is stored in `en_opt_best_model` with lambda = `en_opt_best_lambda`
```

### Sensitivity Analysis - Subset

```{r}
# run elastic net regression on subset training data to collect coefficients
en_subset <- cv.glmnet(x = X_train_subset, y = y_train, alpha = 0.5, family = "binomial", type.measure = "class")

# get optimal lambda for elastic net subset
en_lambda_min_subset <- en_subset$lambda.min
cat("Optimal Lambda for Elastic Net (Subset):", en_lambda_min_subset, "\n")

# plot misclassification error against log lambda for subset
plot(en_subset)
title(main = "Cross-Validation Misclassification Error (Elastic Net - Subset)",
      sub = "Optimal Lambda Highlighted in Red",
      xlab = "Log(Lambda)",
      ylab = "Misclassification Error")

# plot coefficient shrinkage for subset
plot(en_subset$glmnet.fit, xvar = "lambda", label = TRUE)
title(main = "Coefficient Shrinkage Path (Elastic Net - Subset)",
      sub = "Optimal Lambda Highlighted in Red",
      xlab = "Log(Lambda)",
      ylab = "Coefficients")

# get fitted probabilities for subset
en_predict_subset <- predict(en_subset, s = en_lambda_min_subset, X_test_subset, type = "response")

# add variable for predicted classes for subset
en_y_hat_subset <- ifelse(en_predict_subset > 0.5, 1, 0)

# plot confusion matrix for en_subset regression out of sample
generate_cm(y_test, en_y_hat_subset, "Confusion Matrix for Elastic Net (Subset Model)")

# compute roc object for subset ridge model
en_roc_subset <- roc(response = y_test, predictor = en_predict_subset)

# plot combined roc curve for ridge models
plot_combined_roc(en_roc_full, en_roc_subset, "Elastic Net Full", "Elastic Net Subset")
```

## Variable Importance

```{r}
# insert most important coef from the models here
```

# Random Forest

Next, we implement the random forest algorithm [justification].

We do this on the subset of the data as.

We tune the maximum number features of the tree first.

To do this, we split our current training data set into a training set and validation set.

```{r}
# subset training data for validation split
n_train <- nrow(X_train)
id_split <- sample(1:n_train, floor(0.5 * n_train))

# split into training and validation sets
X_train_subset_final <- X_train_subset[id_split, ]
y_train_final <- y_train[id_split]
X_val_subset <- X_train_subset[-id_split, ]
y_val <- y_train[-id_split]

# print dimensions
cat("dimensions of X_train_final:", dim(X_train_subset_final), "\n")
cat("dimensions of y_train_final:", length(y_train_final), "\n")
cat("dimensions of X_val:", dim(X_val_subset), "\n")
cat("dimensions of y_val:", length(y_val), "\n")

# random forest tuning parameters
ntree <- 100 # can optimize over thislater

# we also use the default for bootstrap sample size, i.e nrow(X_train_final)

# initialize performance storage
train_acc <- test_acc <- oob_acc <- ncol(X_train_subset_final)

# tune random forest for max_features
for (max_features in 1:ncol(X_train_subset_final)) { # setting this < ncol(X_train_subset_final for now, just to check if it runs)
  
    # fit the rf model using training data
    rf <- randomForest(x = X_train_subset_final, y = factor(y_train_final), mtry = max_features, ntree = ntree)

    # use trained model to predict y with validation x
    y_pred_val <- predict(rf, X_val_subset)

    # use trained model to predict y with training x
    y_pred_train <- rf$predicted
    
    # compute OOB accuracy measure - OOB error rate
    oob_acc[max_features] <- 1 - rf$err.rate[ntree, "OOB"]

    # compute training accuracy
    train_acc[max_features] <- mean(y_train_final == as.numeric(levels(y_pred_train)[y_pred_train]))
    
    # compute test (val) accuracy - key parameter of interest
    test_acc[max_features] <- mean(y_val == as.numeric(levels(y_pred_val)[y_pred_val]))
}

# identify best max_features in validation data
m_star <- which.max(test_acc)
cat("optimal mtry (max_features):", m_star, "\n")

# plot accuracy metrics
n_features <- length(train_acc) # Should be 15 based on initialization
plot(seq_len(n_features), train_acc, type = 'l', main = 'Tuning mtry for Subset', 
     ylab = 'Accuracy', xlab = 'Number of Features', col = 'blue')
lines(seq_len(n_features), test_acc, col = 'red')
lines(seq_len(n_features), oob_acc, lty = 2, col = 'green')
legend('bottomright', legend = c('Train Accuracy', 'Validation Accuracy', 'OOB Accuracy'), 
       col = c('blue', 'red', 'green'), lty = c(1, 1, 2), bty = 'n')

# train final random forest model with optimal m
RFTuned <- randomForest(x = X_train_subset_final, y = factor(y_train_final), mtry = m_star, ntree = ntree)

# predict using tuned model and test x
RFPred <- predict(RFTuned, newdata = X_test_subset)

# compute fitted probabilities with tuned model and test x
RFProb <- predict(RFTuned, newdata = X_test_subset, type = "prob")

# compute roc and auc for random forest
RF_roc <- roc(response = factor(y_test), predictor = RFProb[, 2], levels = c(0, 1))
RF_auc <- round(auc(RF_roc), 4)
cat("Random Forest AUC (Subset):", RF_auc, "\n")

# plot roc
plot(y = RF_roc$sensitivities, x = 1 - RF_roc$specificities, type = 'l',
     col = 'darkblue', lwd = 2, xlab = 'False Positive Rate', ylab = 'True Positive Rate',
     main = 'ROC Curve for Random Forest (Subset)')
abline(a = 0, b = 1, lty = 2, col = 'gray')
grid()
legend('bottomright',
       legend = c(sprintf("Random Forest AUC: %.3f", auc(RF_roc)), 'Random Guess'),
       lty = c(1, 2), lwd = 2, col = c('darkblue', 'gray'))
box()


# generate confusion matrix
generate_cm(
  true = y_test,
  predicted = RFPred,
  title = "Confusion Matrix for Random Forest (Subset)"
)

```

## Tuning ntree

if we have computation capacity

## Variable Importance

```{r}
# Extract Random Forest importance
rf_importance_subset <- data.frame(
  Variable = rownames(RFTuned$importance),
  Scaled_Importance = RFTuned$importance[, "MeanDecreaseGini"] / max(RFTuned$importance[, "MeanDecreaseGini"], na.rm = TRUE)
)
rf_importance_subset <- rf_importance_subset %>%
  arrange(desc(Scaled_Importance))

# plot rf variable importance - top 20
ggplot(rf_importance_subset %>% arrange(desc(Scaled_Importance)) %>% slice(1:20), 
       aes(x = reorder(Variable, Scaled_Importance), y = Scaled_Importance)) +
  geom_bar(stat = "identity", fill = "blue", alpha = 0.7) +
  coord_flip() +
  labs(title = "Top 20 Random Forest Variable Importance (Subset)",
       x = "Variable",
       y = "Scaled Importance") +
  theme_minimal()

```

Insert interpretation on importance

Relative to lasso etc.

# Conclusion

make a final graph here with the ROCs for each model on the same graph, or a table of AUCs for each model, to make it easier to compare rather than going back and looking at at separate graphs etc